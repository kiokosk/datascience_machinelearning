{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5420dc71-c5c2-4eae-8b98-0a8db3ced519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1: Classification of Fully Connected Layers\n",
    "class FC:\n",
    "    \"\"\"\n",
    "    Fully connected layer from n_nodes1 to n_nodes2 nodes\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "        Number of nodes in the previous layer\n",
    "    n_nodes2 : int\n",
    "        Number of nodes in the next layer\n",
    "    initializer : instance of initialization method\n",
    "    optimizer : instance of optimization method\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # Initialization\n",
    "        # Use initializer's methods to initialize self.W and self.B\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (batch_size, n_nodes1)\n",
    "            Input\n",
    "        Returns\n",
    "        -------\n",
    "        A : ndarray of shape (batch_size, n_nodes2)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        self.X = X  # Store input for backpropagation\n",
    "        # A = XÂ·W + B\n",
    "        A = X @ self.W + self.B\n",
    "        return A\n",
    "        \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward pass\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray of shape (batch_size, n_nodes2)\n",
    "            Gradient flowing from the next layer\n",
    "        Returns\n",
    "        -------\n",
    "        dZ : ndarray of shape (batch_size, n_nodes1)\n",
    "            Gradient to pass to the previous layer\n",
    "        \"\"\"\n",
    "        batch_size = dA.shape[0]\n",
    "        \n",
    "        # Gradient with respect to W\n",
    "        self.dW = self.X.T @ dA / batch_size\n",
    "        \n",
    "        # Gradient with respect to B\n",
    "        self.dB = np.sum(dA, axis=0) / batch_size\n",
    "        \n",
    "        # Gradient with respect to X\n",
    "        dZ = dA @ self.W.T\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "274ba84a-9246-47d6-98c2-8c1058b590cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2: Classifying the Initialization Method\n",
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    Simple initialization using Gaussian distribution\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "        Standard deviation of the Gaussian distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Weight initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "            Number of nodes in the previous layer\n",
    "        n_nodes2 : int\n",
    "            Number of nodes in the next layer\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        W : ndarray of shape (n_nodes1, n_nodes2)\n",
    "            Initialized weights\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "        \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "            Number of nodes in the next layer\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        B : ndarray of shape (n_nodes2,)\n",
    "            Initialized biases\n",
    "        \"\"\"\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25827ddb-040d-41fc-87a7-cc3c948b6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3: Classification of Optimization Methods\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update weights and biases of a layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : instance of a layer\n",
    "            Layer to update\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        layer : instance of a layer\n",
    "            Updated layer\n",
    "        \"\"\"\n",
    "        # Update weights\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        \n",
    "        # Update biases\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c05f6ec9-ae28-4dce-a662-48aed858ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 4: Classification of Activation Functions\n",
    "class Tanh:\n",
    "    \"\"\"\n",
    "    Hyperbolic tangent activation function\n",
    "    \"\"\"\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            Input\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Y : ndarray\n",
    "            Output after applying tanh\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        return np.tanh(X)\n",
    "        \n",
    "    def backward(self, dY):\n",
    "        \"\"\"\n",
    "        Backward pass\n",
    "        Parameters\n",
    "        ----------\n",
    "        dY : ndarray\n",
    "            Gradient from the next layer\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dX : ndarray\n",
    "            Gradient to pass to the previous layer\n",
    "        \"\"\"\n",
    "        # Derivative of tanh(x) is 1 - tanh^2(x)\n",
    "        dX = dY * (1 - np.tanh(self.X)**2)\n",
    "        return dX\n",
    "\n",
    "class Softmax:\n",
    "    \"\"\"\n",
    "    Softmax activation function with cross-entropy loss\n",
    "    \"\"\"\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (batch_size, n_classes)\n",
    "            Input\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Y : ndarray of shape (batch_size, n_classes)\n",
    "            Output probabilities after applying softmax\n",
    "        \"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        self.X = X\n",
    "        exp_X = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        self.Y = exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
    "        return self.Y\n",
    "        \n",
    "    def backward(self, T, Y=None):\n",
    "        \"\"\"\n",
    "        Backward pass (combined with cross-entropy loss)\n",
    "        Parameters\n",
    "        ----------\n",
    "        T : ndarray of shape (batch_size, n_classes)\n",
    "            True labels (one-hot encoded)\n",
    "        Y : ndarray of shape (batch_size, n_classes), optional\n",
    "            Output probabilities from forward pass\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dX : ndarray of shape (batch_size, n_classes)\n",
    "            Gradient to pass to the previous layer\n",
    "        \"\"\"\n",
    "        if Y is None:\n",
    "            Y = self.Y\n",
    "            \n",
    "        batch_size = Y.shape[0]\n",
    "        # Derivative of softmax with cross-entropy is (y - t)\n",
    "        dX = (Y - T) / batch_size\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4e3b4d4-d6bf-4466-819d-de7f205d3777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 5: Creating a ReLU Class\n",
    "class ReLU:\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit activation function\n",
    "    \"\"\"\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            Input\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Y : ndarray\n",
    "            Output after applying ReLU\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        return np.maximum(0, X)\n",
    "        \n",
    "    def backward(self, dY):\n",
    "        \"\"\"\n",
    "        Backward pass\n",
    "        Parameters\n",
    "        ----------\n",
    "        dY : ndarray\n",
    "            Gradient from the next layer\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dX : ndarray\n",
    "            Gradient to pass to the previous layer\n",
    "        \"\"\"\n",
    "        # Derivative of ReLU is 1 if x > 0, 0 otherwise\n",
    "        dX = dY * (self.X > 0)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b9767e5-6cf6-408c-96de-7f6dea30bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 6: Initial Weight Values\n",
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavier (Glorot) initialization\n",
    "    Suitable for sigmoid and tanh activation functions\n",
    "    \"\"\"\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Weight initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "            Number of nodes in the previous layer\n",
    "        n_nodes2 : int\n",
    "            Number of nodes in the next layer\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        W : ndarray of shape (n_nodes1, n_nodes2)\n",
    "            Initialized weights\n",
    "        \"\"\"\n",
    "        # Xavier initialization: sigma = 1/sqrt(n)\n",
    "        sigma = 1.0 / np.sqrt(n_nodes1)\n",
    "        W = sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "        \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "            Number of nodes in the next layer\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        B : ndarray of shape (n_nodes2,)\n",
    "            Initialized biases\n",
    "        \"\"\"\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B\n",
    "\n",
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    He initialization\n",
    "    Suitable for ReLU activation functions\n",
    "    \"\"\"\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Weight initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "            Number of nodes in the previous layer\n",
    "        n_nodes2 : int\n",
    "            Number of nodes in the next layer\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        W : ndarray of shape (n_nodes1, n_nodes2)\n",
    "            Initialized weights\n",
    "        \"\"\"\n",
    "        # He initialization: sigma = sqrt(2/n)\n",
    "        sigma = np.sqrt(2.0 / n_nodes1)\n",
    "        W = sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "        \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "            Number of nodes in the next layer\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        B : ndarray of shape (n_nodes2,)\n",
    "            Initialized biases\n",
    "        \"\"\"\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a18df9-65d5-4da2-8d38-86ea89f030a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 7: Optimization Methods\n",
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    Adaptive Gradient Algorithm (AdaGrad)\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.h = {}  # Dictionary to store accumulated squared gradients\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update weights and biases of a layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : instance of a layer\n",
    "            Layer to update\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        layer : instance of a layer\n",
    "            Updated layer\n",
    "        \"\"\"\n",
    "        # Initialize accumulated squared gradients if not already present\n",
    "        if id(layer) not in self.h:\n",
    "            self.h[id(layer)] = {\n",
    "                'W': np.zeros_like(layer.W),\n",
    "                'B': np.zeros_like(layer.B)\n",
    "            }\n",
    "        \n",
    "        # Update accumulated squared gradients for weights\n",
    "        self.h[id(layer)]['W'] += layer.dW ** 2\n",
    "        \n",
    "        # Update weights\n",
    "        layer.W -= self.lr * layer.dW / (np.sqrt(self.h[id(layer)]['W']) + 1e-7)\n",
    "        \n",
    "        # Update accumulated squared gradients for biases\n",
    "        self.h[id(layer)]['B'] += layer.dB ** 2\n",
    "        \n",
    "        # Update biases\n",
    "        layer.B -= self.lr * layer.dB / (np.sqrt(self.h[id(layer)]['B']) + 1e-7)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b51f3a7-9824-4595-b55f-7f65b60a800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 8: Completing the Deep Neural Network Class\n",
    "import numpy as np\n",
    "\n",
    "class ScratchDeepNeuralNetworkClassifier:\n",
    "    \"\"\"\n",
    "    Deep Neural Network Classifier from scratch\n",
    "    Parameters\n",
    "    ----------\n",
    "    hidden_layer_sizes : list\n",
    "        List of integers representing the number of nodes in each hidden layer\n",
    "    activation : str, default='tanh'\n",
    "        Activation function to use ('tanh', 'relu')\n",
    "    optimizer : str, default='sgd'\n",
    "        Optimization method ('sgd', 'adagrad')\n",
    "    initializer : str, default='simple'\n",
    "        Weight initialization method ('simple', 'xavier', 'he')\n",
    "    learning_rate : float, default=0.1\n",
    "        Learning rate for optimization\n",
    "    sigma : float, default=0.01\n",
    "        Standard deviation for simple initialization\n",
    "    max_epochs : int, default=1000\n",
    "        Maximum number of epochs\n",
    "    batch_size : int, default=32\n",
    "        Batch size for mini-batch gradient descent\n",
    "    random_state : int, default=None\n",
    "        Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_layer_sizes, activation='tanh', optimizer='sgd',\n",
    "                 initializer='simple', learning_rate=0.1, sigma=0.01,\n",
    "                 max_epochs=1000, batch_size=32, random_state=None):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.optimizer = optimizer\n",
    "        self.initializer = initializer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.sigma = sigma\n",
    "        self.max_epochs = max_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def _get_activation(self, activation_name):\n",
    "        \"\"\"\n",
    "        Get activation function instance\n",
    "        Parameters\n",
    "        ----------\n",
    "        activation_name : str\n",
    "            Name of the activation function\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        activation : instance of activation function\n",
    "        \"\"\"\n",
    "        if activation_name == 'tanh':\n",
    "            return Tanh()\n",
    "        elif activation_name == 'relu':\n",
    "            return ReLU()\n",
    "        elif activation_name == 'softmax':\n",
    "            return Softmax()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation_name}\")\n",
    "    \n",
    "    def _get_initializer(self, initializer_name):\n",
    "        \"\"\"\n",
    "        Get initializer instance\n",
    "        Parameters\n",
    "        ----------\n",
    "        initializer_name : str\n",
    "            Name of the initializer\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        initializer : instance of initializer\n",
    "        \"\"\"\n",
    "        if initializer_name == 'simple':\n",
    "            return SimpleInitializer(self.sigma)\n",
    "        elif initializer_name == 'xavier':\n",
    "            return XavierInitializer()\n",
    "        elif initializer_name == 'he':\n",
    "            return HeInitializer()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported initializer: {initializer_name}\")\n",
    "    \n",
    "    def _get_optimizer(self, optimizer_name):\n",
    "        \"\"\"\n",
    "        Get optimizer instance\n",
    "        Parameters\n",
    "        ----------\n",
    "        optimizer_name : str\n",
    "            Name of the optimizer\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        optimizer : instance of optimizer\n",
    "        \"\"\"\n",
    "        if optimizer_name == 'sgd':\n",
    "            return SGD(self.learning_rate)\n",
    "        elif optimizer_name == 'adagrad':\n",
    "            return AdaGrad(self.learning_rate)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "    \n",
    "    def _init_layers(self, n_features, n_classes):\n",
    "        \"\"\"\n",
    "        Initialize network layers\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_features : int\n",
    "            Number of input features\n",
    "        n_classes : int\n",
    "            Number of output classes\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # Get initializer and optimizer instances\n",
    "        initializer = self._get_initializer(self.initializer)\n",
    "        optimizer = self._get_optimizer(self.optimizer)\n",
    "        \n",
    "        # Create layers\n",
    "        self.layers = []\n",
    "        \n",
    "        # Input layer to first hidden layer\n",
    "        layer_sizes = [n_features] + self.hidden_layer_sizes + [n_classes]\n",
    "        activation_list = [self.activation] * len(self.hidden_layer_sizes) + ['softmax']\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Add fully connected layer\n",
    "            fc = FC(layer_sizes[i], layer_sizes[i+1], initializer, optimizer)\n",
    "            self.layers.append(('fc', fc))\n",
    "            \n",
    "            # Add activation function except for the last layer (handled separately)\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                activation = self._get_activation(activation_list[i])\n",
    "                self.layers.append(('activation', activation))\n",
    "        \n",
    "        # Output layer activation (softmax)\n",
    "        self.output_activation = self._get_activation('softmax')\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the neural network to the training data\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Target values\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        \n",
    "        # Initialize layers\n",
    "        self._init_layers(n_features, n_classes)\n",
    "        \n",
    "        # One-hot encode the target values\n",
    "        T = np.zeros((n_samples, n_classes))\n",
    "        T[np.arange(n_samples), y] = 1\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(self.max_epochs):\n",
    "            # Shuffle data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            T_shuffled = T[indices]\n",
    "            \n",
    "            # Mini-batch training\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X_shuffled[i:i+self.batch_size]\n",
    "                T_batch = T_shuffled[i:i+self.batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                A = X_batch\n",
    "                for layer_type, layer in self.layers:\n",
    "                    A = layer.forward(A)\n",
    "                \n",
    "                Y = self.output_activation.forward(A)\n",
    "                \n",
    "                # Backward pass\n",
    "                dA = self.output_activation.backward(T_batch, Y)\n",
    "                \n",
    "                for layer_type, layer in reversed(self.layers):\n",
    "                    dA = layer.backward(dA)\n",
    "            \n",
    "            # Calculate and print loss occasionally\n",
    "            if epoch % 100 == 0:\n",
    "                loss = self._compute_loss(X, T)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _compute_loss(self, X, T):\n",
    "        \"\"\"\n",
    "        Compute the cross-entropy loss\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Input data\n",
    "        T : ndarray of shape (n_samples, n_classes)\n",
    "            One-hot encoded target values\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "            Cross-entropy loss\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        A = X\n",
    "        for layer_type, layer in self.layers:\n",
    "            A = layer.forward(A)\n",
    "        \n",
    "        Y = self.output_activation.forward(A)\n",
    "        \n",
    "        # Calculate cross-entropy loss\n",
    "        eps = 1e-7\n",
    "        loss = -np.sum(T * np.log(Y + eps)) / len(X)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Input data\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : ndarray of shape (n_samples,)\n",
    "            Predicted class labels\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        A = X\n",
    "        for layer_type, layer in self.layers:\n",
    "            A = layer.forward(A)\n",
    "        \n",
    "        # Get probabilities\n",
    "        Y = self.output_activation.forward(A)\n",
    "        \n",
    "        # Get class with highest probability\n",
    "        y_pred = np.argmax(Y, axis=1)\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate accuracy\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Input data\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            True class labels\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        accuracy : float\n",
    "            Classification accuracy\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = np.mean(y_pred == y)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73f0c8eb-bdf2-474a-8d02-b8c163c68e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset...\n",
      "Training data shape: (56000, 784)\n",
      "Test data shape: (14000, 784)\n",
      "\n",
      "Evaluating Basic Network (1 layer, Tanh, SGD)...\n",
      "Configuration: [100], tanh, sgd, xavier\n",
      "Epoch 0, Loss: 2.298116\n",
      "Epoch 100, Loss: 0.623967\n",
      "Epoch 200, Loss: 0.461936\n",
      "Training time: 77.54 seconds\n",
      "Test accuracy: 0.8923\n",
      "\n",
      "Evaluating Deep Network (2 layers, Tanh, SGD)...\n",
      "Configuration: [100, 100], tanh, sgd, xavier\n",
      "Epoch 0, Loss: 2.311108\n",
      "Epoch 100, Loss: 0.576857\n",
      "Epoch 200, Loss: 0.413986\n",
      "Training time: 123.49 seconds\n",
      "Test accuracy: 0.8996\n",
      "\n",
      "Evaluating ReLU Network (1 layer, ReLU, SGD)...\n",
      "Configuration: [100], relu, sgd, he\n",
      "Epoch 0, Loss: 2.325059\n",
      "Epoch 100, Loss: 0.543723\n",
      "Epoch 200, Loss: 0.406492\n",
      "Training time: 53.14 seconds\n",
      "Test accuracy: 0.9005\n",
      "\n",
      "Evaluating Deep ReLU Network (2 layers, ReLU, SGD)...\n",
      "Configuration: [100, 100], relu, sgd, he\n",
      "Epoch 0, Loss: 2.355095\n",
      "Epoch 100, Loss: 0.456876\n",
      "Epoch 200, Loss: 0.332186\n",
      "Training time: 76.61 seconds\n",
      "Test accuracy: 0.9097\n",
      "\n",
      "Evaluating AdaGrad Network (1 layer, ReLU, AdaGrad)...\n",
      "Configuration: [100], relu, adagrad, he\n",
      "Epoch 0, Loss: 2.343955\n",
      "Epoch 100, Loss: 0.001748\n",
      "Epoch 200, Loss: 0.000729\n",
      "Training time: 67.39 seconds\n",
      "Test accuracy: 0.9531\n",
      "\n",
      "Summary of Results:\n",
      "============================================================\n",
      "Network Configuration                    Accuracy   Time (s)  \n",
      "------------------------------------------------------------\n",
      "Basic Network (1 layer, Tanh, SGD)       0.8923    77.54\n",
      "Deep Network (2 layers, Tanh, SGD)       0.8996    123.49\n",
      "ReLU Network (1 layer, ReLU, SGD)        0.9005    53.14\n",
      "Deep ReLU Network (2 layers, ReLU, SGD)  0.9097    76.61\n",
      "AdaGrad Network (1 layer, ReLU, AdaGrad) 0.9531    67.39\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Problem 9: Learning and Estimation with MNIST\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "def load_mnist():\n",
    "    \"\"\"\n",
    "    Load and preprocess the MNIST dataset\n",
    "    \"\"\"\n",
    "    print(\"Loading MNIST dataset...\")\n",
    "    \n",
    "    # Load data\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    # Reshape and scale features to [0, 1]\n",
    "    X_train, X_test = X_train.reshape(-1, 28*28) / 255.0, X_test.reshape(-1, 28*28) / 255.0\n",
    "    \n",
    "    # Combine train and test sets for splitting\n",
    "    X = np.vstack((X_train, X_test))  # Stack them together\n",
    "    y = np.hstack((y_train, y_test))  # Stack labels\n",
    "    \n",
    "    # Split data into new train-test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def evaluate_network(network_config, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate a neural network with the given configuration\n",
    "    \"\"\"\n",
    "    # Unpack configuration\n",
    "    name = network_config['name']\n",
    "    hidden_layer_sizes = network_config['hidden_layer_sizes']\n",
    "    activation = network_config['activation']\n",
    "    optimizer = network_config['optimizer']\n",
    "    initializer = network_config['initializer']\n",
    "    \n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    print(f\"Configuration: {hidden_layer_sizes}, {activation}, {optimizer}, {initializer}\")\n",
    "    \n",
    "    # Create and train the network\n",
    "    start_time = time.time()\n",
    "    \n",
    "    nn = ScratchDeepNeuralNetworkClassifier(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=activation,\n",
    "        optimizer=optimizer,\n",
    "        initializer=initializer,\n",
    "        learning_rate=0.1,\n",
    "        max_epochs=300, \n",
    "        batch_size=128,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "\n",
    "    subset_size = 10000 \n",
    "    X_train_subset = X_train[:subset_size]\n",
    "    y_train_subset = y_train[:subset_size]\n",
    "    \n",
    "    nn.fit(X_train_subset, y_train_subset)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_score = nn.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"Training time: {training_time:.2f} seconds\")\n",
    "    print(f\"Test accuracy: {test_score:.4f}\")\n",
    "    \n",
    "    return test_score, training_time\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess MNIST data\n",
    "    X_train, X_test, y_train, y_test = load_mnist()\n",
    "    \n",
    "    # Define different network configurations\n",
    "    configurations = [\n",
    "        {\n",
    "            'name': 'Basic Network (1 layer, Tanh, SGD)',\n",
    "            'hidden_layer_sizes': [100],\n",
    "            'activation': 'tanh',\n",
    "            'optimizer': 'sgd',\n",
    "            'initializer': 'xavier'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Deep Network (2 layers, Tanh, SGD)',\n",
    "            'hidden_layer_sizes': [100, 100],\n",
    "            'activation': 'tanh',\n",
    "            'optimizer': 'sgd',\n",
    "            'initializer': 'xavier'\n",
    "        },\n",
    "        {\n",
    "            'name': 'ReLU Network (1 layer, ReLU, SGD)',\n",
    "            'hidden_layer_sizes': [100],\n",
    "            'activation': 'relu',\n",
    "            'optimizer': 'sgd',\n",
    "            'initializer': 'he'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Deep ReLU Network (2 layers, ReLU, SGD)',\n",
    "            'hidden_layer_sizes': [100, 100],\n",
    "            'activation': 'relu',\n",
    "            'optimizer': 'sgd',\n",
    "            'initializer': 'he'\n",
    "        },\n",
    "        {\n",
    "            'name': 'AdaGrad Network (1 layer, ReLU, AdaGrad)',\n",
    "            'hidden_layer_sizes': [100],\n",
    "            'activation': 'relu',\n",
    "            'optimizer': 'adagrad',\n",
    "            'initializer': 'he'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Evaluate each configuration\n",
    "    results = []\n",
    "    for config in configurations:\n",
    "        accuracy, training_time = evaluate_network(config, X_train, X_test, y_train, y_test)\n",
    "        results.append((config['name'], accuracy, training_time))\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nSummary of Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Network Configuration':<40} {'Accuracy':<10} {'Time (s)':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    for name, accuracy, time in results:\n",
    "        print(f\"{name:<40} {accuracy:.4f}    {time:.2f}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78b1773-55d9-4350-a945-3391a12a5e23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
