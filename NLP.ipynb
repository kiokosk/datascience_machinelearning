{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5abdd683-38e5-41b4-88e4-05d1539be1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset...\n",
      "Loaded 25000 training samples and 25000 test samples\n",
      "Labels: ['neg', 'pos']\n",
      "\n",
      "Sample review:\n",
      "Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mu ...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Load the IMDB dataset\n",
    "print(\"Loading IMDB dataset...\")\n",
    "\n",
    "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "x_train, y_train = train_review.data, train_review.target\n",
    "\n",
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "x_test, y_test = test_review.data, test_review.target\n",
    "\n",
    "print(f\"Loaded {len(x_train)} training samples and {len(x_test)} test samples\")\n",
    "print(f\"Labels: {train_review.target_names}\")\n",
    "\n",
    "# Preview a sample\n",
    "print(\"\\nSample review:\")\n",
    "print(x_train[0][:300], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b089cd-76e7-44ef-876d-f021e5906b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===== Problem 1: Scratch implementation of BoW =====\n",
      "Unigram Vocabulary: ['a', 'best', 'ever', 'funny', 'i', 'is', 'movie', 'never', 'soooo', 'this', 'what']\n",
      "\n",
      "Unigram BoW Matrix:\n",
      "Sentence 1: {'funny': 1, 'is': 1, 'movie': 1, 'soooo': 1, 'this': 1}\n",
      "Sentence 2: {'a': 1, 'i': 1, 'movie': 1, 'never': 1, 'what': 1}\n",
      "Sentence 3: {'best': 1, 'ever': 1, 'movie': 2, 'this': 1}\n",
      "\n",
      "Bigram Vocabulary: ['a', 'a movie', 'best', 'best movie', 'ever', 'ever this', 'funny', 'i', 'i never', 'is', 'is soooo', 'movie', 'movie ever', 'movie i', 'movie is', 'never', 'soooo', 'soooo funny', 'this', 'this movie', 'what', 'what a']\n",
      "\n",
      "Bigram BoW Matrix:\n",
      "Sentence 1: {'is soooo': 1, 'movie is': 1, 'soooo funny': 1, 'this movie': 1}\n",
      "Sentence 2: {'a movie': 1, 'i never': 1, 'movie i': 1, 'what a': 1}\n",
      "Sentence 3: {'best movie': 1, 'ever this': 1, 'movie ever': 1, 'this movie': 1}\n"
     ]
    }
   ],
   "source": [
    "# Problem 1: Scratch implementation of BoW\n",
    "print(\"\\n\\n===== Problem 1: Scratch implementation of BoW =====\")\n",
    "\n",
    "sentences = [\n",
    "    \"This movie is SOOOO funny!!!\",\n",
    "    \"What a movie! I never\",\n",
    "    \"best movie ever!!!!! this movie\"\n",
    "]\n",
    "\n",
    "def create_bow(sentences, use_bigrams=False):\n",
    "    # Preprocessing - convert to lowercase and remove punctuation\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Convert to lowercase\n",
    "        sentence = sentence.lower()\n",
    "        # Remove punctuation\n",
    "        sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "        processed_sentences.append(sentence)\n",
    "    \n",
    "    # Create vocabulary\n",
    "    vocabulary = set()\n",
    "    for sentence in processed_sentences:\n",
    "        words = sentence.split()\n",
    "        \n",
    "        # Add unigrams to vocabulary\n",
    "        vocabulary.update(words)\n",
    "        \n",
    "        # Add bigrams to vocabulary if required\n",
    "        if use_bigrams:\n",
    "            bigrams = [words[i] + \" \" + words[i+1] for i in range(len(words)-1)]\n",
    "            vocabulary.update(bigrams)\n",
    "    \n",
    "    # Convert vocabulary to ordered list\n",
    "    vocabulary = sorted(list(vocabulary))\n",
    "    \n",
    "    # Create BoW representation\n",
    "    bow_matrix = []\n",
    "    for sentence in processed_sentences:\n",
    "        words = sentence.split()\n",
    "        \n",
    "        # Count unigrams\n",
    "        bow_vector = {word: 0 for word in vocabulary}\n",
    "        for word in words:\n",
    "            if word in bow_vector:\n",
    "                bow_vector[word] += 1\n",
    "        \n",
    "        # Count bigrams if required\n",
    "        if use_bigrams:\n",
    "            bigrams = [words[i] + \" \" + words[i+1] for i in range(len(words)-1)]\n",
    "            for bigram in bigrams:\n",
    "                if bigram in bow_vector:\n",
    "                    bow_vector[bigram] += 1\n",
    "        \n",
    "        bow_matrix.append(bow_vector)\n",
    "    \n",
    "    return bow_matrix, vocabulary\n",
    "\n",
    "# Compute unigram BoW\n",
    "unigram_bow, unigram_vocab = create_bow(sentences, use_bigrams=False)\n",
    "print(\"Unigram Vocabulary:\", unigram_vocab)\n",
    "print(\"\\nUnigram BoW Matrix:\")\n",
    "for i, vec in enumerate(unigram_bow):\n",
    "    print(f\"Sentence {i+1}:\", {word: count for word, count in vec.items() if count > 0})\n",
    "\n",
    "# Compute bigram BoW\n",
    "bigram_bow, bigram_vocab = create_bow(sentences, use_bigrams=True)\n",
    "print(\"\\nBigram Vocabulary:\", bigram_vocab)\n",
    "print(\"\\nBigram BoW Matrix:\")\n",
    "for i, vec in enumerate(bigram_bow):\n",
    "    print(f\"Sentence {i+1}:\", {word: count for word, count in vec.items() if count > 0 and \" \" in word})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eea9322-5b6e-48e4-b5db-e8d2309fa417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===== Problem 2: Calculating TF-IDF =====\n",
      "Training data shape: (25000, 5000)\n",
      "Test data shape: (25000, 5000)\n",
      "Vocabulary size: 5000\n",
      "\n",
      "Sample vocabulary words:\n",
      "['00', '000', '10', '100', '11', '12', '13', '13th', '14', '15', '16', '17', '18', '1930', '1930s', '1933', '1940', '1950', '1950s', '1960']\n"
     ]
    }
   ],
   "source": [
    "# Problem 2: Calculating TF-IDF\n",
    "print(\"\\n\\n===== Problem 2: Calculating TF-IDF =====\")\n",
    "\n",
    "# Get NLTK stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    stop_words=stop_words,  # Use NLTK stopwords\n",
    "    max_features=5000,      # Limit vocabulary size to 5000\n",
    "    ngram_range=(1, 1),     # Use unigrams\n",
    "    norm='l2'               # Apply L2 normalization by default\n",
    ")\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "# Transform the test data using the same vocabulary\n",
    "X_test_tfidf = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "print(f\"Training data shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Test data shape: {X_test_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf_vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "# Preview some of the vocabulary\n",
    "print(\"\\nSample vocabulary words:\")\n",
    "print(list(tfidf_vectorizer.get_feature_names_out()[:20]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "968c4318-25e5-4357-befd-44abc86b92f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===== Problem 3: Learning using TF-IDF =====\n",
      "Experiment 1: Baseline\n",
      "Parameters: {'stop_words': ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"], 'max_features': 5000, 'ngram_range': (1, 1)}\n",
      "Vocabulary size: 5000\n",
      "Vectorization time: 41.48 seconds\n",
      "Training time: 1.78 seconds\n",
      "Prediction time: 0.02 seconds\n",
      "Accuracy: 0.8810\n",
      "--------------------------------------------------------------------------------\n",
      "Experiment 2: Larger vocabulary\n",
      "Parameters: {'stop_words': ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"], 'max_features': 10000, 'ngram_range': (1, 1)}\n",
      "Vocabulary size: 10000\n",
      "Vectorization time: 39.98 seconds\n",
      "Training time: 3.18 seconds\n",
      "Prediction time: 0.01 seconds\n",
      "Accuracy: 0.8828\n",
      "--------------------------------------------------------------------------------\n",
      "Experiment 3: Include bigrams\n",
      "Parameters: {'stop_words': ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"], 'max_features': 5000, 'ngram_range': (1, 2)}\n",
      "Vocabulary size: 5000\n",
      "Vectorization time: 108.07 seconds\n",
      "Training time: 2.59 seconds\n",
      "Prediction time: 0.01 seconds\n",
      "Accuracy: 0.8858\n",
      "--------------------------------------------------------------------------------\n",
      "Experiment 4: No stop words\n",
      "Parameters: {'stop_words': None, 'max_features': 5000, 'ngram_range': (1, 1)}\n",
      "Vocabulary size: 5000\n",
      "Vectorization time: 47.18 seconds\n",
      "Training time: 4.41 seconds\n",
      "Prediction time: 0.03 seconds\n",
      "Accuracy: 0.8825\n",
      "--------------------------------------------------------------------------------\n",
      "Summary of Results:\n",
      "Baseline (5000 features, unigrams, with stopwords): 0.8810\n",
      "Larger vocabulary (10000 features): 0.8828\n",
      "Including bigrams: 0.8858\n",
      "No stopwords: 0.8825\n"
     ]
    }
   ],
   "source": [
    "# Problem 3: Learning using TF-IDF\n",
    "print(\"\\n\\n===== Problem 3: Learning using TF-IDF =====\")\n",
    "\n",
    "def train_and_evaluate(tfidf_vectorizer_params):\n",
    "    # Create TF-IDF vectorizer with given parameters\n",
    "    tfidf_vectorizer = TfidfVectorizer(**tfidf_vectorizer_params)\n",
    "    \n",
    "    # Fit and transform the training data\n",
    "    start_time = time.time()\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(x_test)\n",
    "    vectorize_time = time.time() - start_time\n",
    "    \n",
    "    # Train a logistic regression model\n",
    "    start_time = time.time()\n",
    "    classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    classifier.fit(X_train_tfidf, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    start_time = time.time()\n",
    "    y_pred = classifier.predict(X_test_tfidf)\n",
    "    predict_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Parameters: {tfidf_vectorizer_params}\")\n",
    "    print(f\"Vocabulary size: {len(tfidf_vectorizer.get_feature_names_out())}\")\n",
    "    print(f\"Vectorization time: {vectorize_time:.2f} seconds\")\n",
    "    print(f\"Training time: {train_time:.2f} seconds\")\n",
    "    print(f\"Prediction time: {predict_time:.2f} seconds\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Experiment 1: Baseline with default settings\n",
    "print(\"Experiment 1: Baseline\")\n",
    "baseline_params = {\n",
    "    'stop_words': stopwords.words('english'),\n",
    "    'max_features': 5000,\n",
    "    'ngram_range': (1, 1)\n",
    "}\n",
    "baseline_accuracy = train_and_evaluate(baseline_params)\n",
    "\n",
    "# Experiment 2: Increase vocabulary size\n",
    "print(\"Experiment 2: Larger vocabulary\")\n",
    "large_vocab_params = {\n",
    "    'stop_words': stopwords.words('english'),\n",
    "    'max_features': 10000,\n",
    "    'ngram_range': (1, 1)\n",
    "}\n",
    "large_vocab_accuracy = train_and_evaluate(large_vocab_params)\n",
    "\n",
    "# Experiment 3: Include bigrams\n",
    "print(\"Experiment 3: Include bigrams\")\n",
    "bigram_params = {\n",
    "    'stop_words': stopwords.words('english'),\n",
    "    'max_features': 5000,\n",
    "    'ngram_range': (1, 2)\n",
    "}\n",
    "bigram_accuracy = train_and_evaluate(bigram_params)\n",
    "\n",
    "# Experiment 4: No stop words\n",
    "print(\"Experiment 4: No stop words\")\n",
    "no_stopwords_params = {\n",
    "    'stop_words': None,\n",
    "    'max_features': 5000,\n",
    "    'ngram_range': (1, 1)\n",
    "}\n",
    "no_stopwords_accuracy = train_and_evaluate(no_stopwords_params)\n",
    "\n",
    "# Summarize results\n",
    "print(\"Summary of Results:\")\n",
    "print(f\"Baseline (5000 features, unigrams, with stopwords): {baseline_accuracy:.4f}\")\n",
    "print(f\"Larger vocabulary (10000 features): {large_vocab_accuracy:.4f}\")\n",
    "print(f\"Including bigrams: {bigram_accuracy:.4f}\")\n",
    "print(f\"No stopwords: {no_stopwords_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a37737da-72b9-4551-8d97-89285874912b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===== Problem 4: Scratch implementation of TF-IDF =====\n",
      "Standard TF-IDF Formula:\n",
      "Sentence 1:\n",
      "  funny: 0.2197\n",
      "  is: 0.2197\n",
      "  soooo: 0.2197\n",
      "  this: 0.0811\n",
      "  a: 0.0000\n",
      "Sentence 2:\n",
      "  a: 0.2197\n",
      "  i: 0.2197\n",
      "  never: 0.2197\n",
      "  what: 0.2197\n",
      "  best: 0.0000\n",
      "Sentence 3:\n",
      "  best: 0.2197\n",
      "  ever: 0.2197\n",
      "  this: 0.0811\n",
      "  a: 0.0000\n",
      "  funny: 0.0000\n",
      "\n",
      "Scikit-learn TF-IDF Formula:\n",
      "Sentence 1:\n",
      "  funny: 1.6931\n",
      "  is: 1.6931\n",
      "  soooo: 1.6931\n",
      "  this: 1.2877\n",
      "  movie: 1.0000\n",
      "Sentence 2:\n",
      "  a: 1.6931\n",
      "  i: 1.6931\n",
      "  never: 1.6931\n",
      "  what: 1.6931\n",
      "  movie: 1.0000\n",
      "Sentence 3:\n",
      "  movie: 2.0000\n",
      "  best: 1.6931\n",
      "  ever: 1.6931\n",
      "  this: 1.2877\n",
      "  a: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Problem 4: Scratch implementation of TF-IDF\n",
    "print(\"\\n\\n===== Problem 4: Scratch implementation of TF-IDF =====\")\n",
    "\n",
    "def compute_tfidf(sentences, standard_formula=True):\n",
    "    # Preprocessing\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Convert to lowercase\n",
    "        sentence = sentence.lower()\n",
    "        # Remove punctuation\n",
    "        sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "        processed_sentences.append(sentence)\n",
    "    \n",
    "    # Tokenize sentences into words\n",
    "    tokenized_sentences = [sentence.split() for sentence in processed_sentences]\n",
    "    \n",
    "    # Build vocabulary\n",
    "    vocabulary = set()\n",
    "    for tokens in tokenized_sentences:\n",
    "        vocabulary.update(tokens)\n",
    "    vocabulary = sorted(list(vocabulary))\n",
    "    \n",
    "    # Calculate term frequency (TF)\n",
    "    term_freq = []\n",
    "    for tokens in tokenized_sentences:\n",
    "        # Count word occurrences\n",
    "        word_counts = {word: 0 for word in vocabulary}\n",
    "        for token in tokens:\n",
    "            if token in word_counts:\n",
    "                word_counts[token] += 1\n",
    "        \n",
    "        if standard_formula:\n",
    "            # Standard formula: tf(t,d) = n(t,d) / sum(n(s,d))\n",
    "            total_words = sum(word_counts.values())\n",
    "            if total_words > 0:  # Avoid division by zero\n",
    "                tf = {word: count / total_words for word, count in word_counts.items()}\n",
    "            else:\n",
    "                tf = word_counts\n",
    "        else:\n",
    "            # scikit-learn formula: tf(t,d) = n(t,d)\n",
    "            tf = word_counts\n",
    "        \n",
    "        term_freq.append(tf)\n",
    "    \n",
    "    # Calculate document frequency (DF)\n",
    "    doc_freq = {word: 0 for word in vocabulary}\n",
    "    for tokens in tokenized_sentences:\n",
    "        # Count documents containing each word\n",
    "        unique_tokens = set(tokens)\n",
    "        for word in unique_tokens:\n",
    "            if word in doc_freq:\n",
    "                doc_freq[word] += 1\n",
    "    \n",
    "    # Calculate inverse document frequency (IDF)\n",
    "    num_docs = len(tokenized_sentences)\n",
    "    if standard_formula:\n",
    "        # Standard formula: idf(t) = log(N / df(t))\n",
    "        idf = {word: math.log(num_docs / df) if df > 0 else 0 for word, df in doc_freq.items()}\n",
    "    else:\n",
    "        # scikit-learn formula: idf(t) = log((1+N)/(1+df(t))) + 1\n",
    "        idf = {word: math.log((1 + num_docs) / (1 + df)) + 1 for word, df in doc_freq.items()}\n",
    "    \n",
    "    # Calculate TF-IDF\n",
    "    tfidf_matrix = []\n",
    "    for tf in term_freq:\n",
    "        tfidf = {word: tf[word] * idf[word] for word in vocabulary}\n",
    "        tfidf_matrix.append(tfidf)\n",
    "    \n",
    "    return tfidf_matrix, vocabulary\n",
    "\n",
    "# Test sentences\n",
    "sentences = [\n",
    "    \"This movie is SOOOO funny!!!\",\n",
    "    \"What a movie! I never\",\n",
    "    \"best movie ever!!!!! this movie\"\n",
    "]\n",
    "\n",
    "# Calculate TF-IDF using standard formula\n",
    "standard_tfidf, vocabulary = compute_tfidf(sentences, standard_formula=True)\n",
    "print(\"Standard TF-IDF Formula:\")\n",
    "for i, tfidf in enumerate(standard_tfidf):\n",
    "    print(f\"Sentence {i+1}:\")\n",
    "    for word, value in sorted(tfidf.items(), key=lambda x: -x[1])[:5]:  # Show top 5 highest values\n",
    "        print(f\"  {word}: {value:.4f}\")\n",
    "\n",
    "# Calculate TF-IDF using scikit-learn formula\n",
    "sklearn_tfidf, _ = compute_tfidf(sentences, standard_formula=False)\n",
    "print(\"\\nScikit-learn TF-IDF Formula:\")\n",
    "for i, tfidf in enumerate(sklearn_tfidf):\n",
    "    print(f\"Sentence {i+1}:\")\n",
    "    for word, value in sorted(tfidf.items(), key=lambda x: -x[1])[:5]:  # Show top 5 highest values\n",
    "        print(f\"  {word}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc22b2b4-56ea-45e3-be9c-4b40779516e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===== Problem 5: Corpus preprocessing =====\n",
      "Number of training reviews: 25000\n",
      "Number of test reviews: 25000\n",
      "Original review (first 150 chars): Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It...\n",
      "Preprocessed review (first 30 tokens): ['zero', 'day', 'leads', 'you', 'to', 'think', 'even', 're', 'think', 'why', 'two', 'boys', 'young', 'men', 'would', 'do', 'what', 'they', 'did', 'commit', 'mutual', 'suicide', 'via', 'slaughtering', 'their', 'classmates', 'it', 'captures', 'what', 'must']...\n"
     ]
    }
   ],
   "source": [
    "# Problem 5: Corpus preprocessing\n",
    "\n",
    "print(\"\\n\\n===== Problem 5: Corpus preprocessing =====\")\n",
    "\n",
    "\n",
    "# def preprocess_text(text):\n",
    "    \n",
    "#     # Convert to lowercase\n",
    "#     text = text.lower()\n",
    "    \n",
    "#     # Remove URLs\n",
    "#     text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "#     # Remove HTML tags\n",
    "#     text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "#     # Remove special characters and numbers\n",
    "#     text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "#     # Tokenize text\n",
    "#     tokens = word_tokenize(text)\n",
    "    \n",
    "#     # Remove extra spaces and empty tokens\n",
    "#     tokens = [token for token in tokens if token.strip()]\n",
    "    \n",
    "#     return tokens\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and replace with spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Simple tokenization by splitting on whitespace\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove empty tokens\n",
    "    tokens = [token for token in tokens if token.strip()]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Preprocess training and testing data\n",
    "preprocessed_train = [preprocess_text(review) for review in x_train]\n",
    "preprocessed_test = [preprocess_text(review) for review in x_test]\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Number of training reviews: {len(preprocessed_train)}\")\n",
    "print(f\"Number of test reviews: {len(preprocessed_test)}\")\n",
    "\n",
    "# Print a sample preprocessed review\n",
    "sample_idx = 0\n",
    "print(f\"Original review (first 150 chars): {x_train[sample_idx][:150]}...\")\n",
    "print(f\"Preprocessed review (first 30 tokens): {preprocessed_train[sample_idx][:30]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdef1aa4-2544-4d55-8037-5013d1a6dc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===== Problem 6: Learning Word2Vec =====\n",
      "Word2Vec model trained in 101.37 seconds\n",
      "Vocabulary size: 28770\n",
      "\n",
      "Vector for 'movie' (first 5 dimensions):\n",
      "[ 0.22120029  0.23028044  0.16105013 -0.04356096 -0.1844663 ]\n",
      "Most similar words to 'movie':\n",
      "  film: 0.9156\n",
      "  programme: 0.7718\n",
      "  flick: 0.7713\n",
      "  monstrosity: 0.7538\n",
      "  loooong: 0.7387\n",
      "\n",
      "Vector for 'good' (first 5 dimensions):\n",
      "[-0.37467164 -0.08395848 -0.28677136  0.08928741 -0.1305031 ]\n",
      "Most similar words to 'good':\n",
      "  decent: 0.8041\n",
      "  great: 0.7842\n",
      "  bad: 0.7632\n",
      "  fine: 0.7312\n",
      "  nice: 0.7229\n",
      "\n",
      "Vector for 'bad' (first 5 dimensions):\n",
      "[-0.14845057 -0.14544067 -0.32181013  0.00300282  0.13404873]\n",
      "Most similar words to 'bad':\n",
      "  terrible: 0.8243\n",
      "  horrible: 0.7884\n",
      "  awful: 0.7787\n",
      "  good: 0.7632\n",
      "  lousy: 0.7443\n",
      "\n",
      "Vector for 'excellent' (first 5 dimensions):\n",
      "[ 0.11832446  0.04754427  0.20652366 -0.05519987 -0.15238181]\n",
      "Most similar words to 'excellent':\n",
      "  outstanding: 0.8729\n",
      "  exceptional: 0.8139\n",
      "  terrific: 0.7853\n",
      "  superb: 0.7769\n",
      "  fantastic: 0.7766\n",
      "\n",
      "Vector for 'terrible' (first 5 dimensions):\n",
      "[-0.06726407 -0.31483784 -0.28709003 -0.11539458 -0.18975827]\n",
      "Most similar words to 'terrible':\n",
      "  horrible: 0.9101\n",
      "  awful: 0.8520\n",
      "  abysmal: 0.8448\n",
      "  horrendous: 0.8363\n",
      "  dreadful: 0.8347\n"
     ]
    }
   ],
   "source": [
    "# Problem 6: Learning Word2Vec\n",
    "print(\"\\n\\n===== Problem 6: Learning Word2Vec =====\")\n",
    "\n",
    "# Set Word2Vec parameters\n",
    "vector_size = 100   # Dimensionality of the word vectors\n",
    "window = 5          # Maximum distance between current and predicted word\n",
    "min_count = 5       # Minimum word frequency to include in vocabulary\n",
    "workers = 4         # Number of CPU cores to use\n",
    "\n",
    "# Train Word2Vec model\n",
    "start_time = time.time()\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=preprocessed_train,\n",
    "    vector_size=vector_size,\n",
    "    window=window,\n",
    "    min_count=min_count,\n",
    "    workers=workers,\n",
    "    sg=1  # Use skip-gram (sg=1) instead of CBOW (sg=0)\n",
    ")\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Print model information\n",
    "print(f\"Word2Vec model trained in {training_time:.2f} seconds\")\n",
    "print(f\"Vocabulary size: {len(w2v_model.wv.key_to_index)}\")\n",
    "\n",
    "# Save the model (optional)\n",
    "# w2v_model.save(\"imdb_word2vec.model\")\n",
    "\n",
    "# Explore some word vectors\n",
    "common_words = ['movie', 'good', 'bad', 'excellent', 'terrible']\n",
    "for word in common_words:\n",
    "    if word in w2v_model.wv:\n",
    "        print(f\"\\nVector for '{word}' (first 5 dimensions):\")\n",
    "        print(w2v_model.wv[word][:5])\n",
    "        \n",
    "        # Find similar words\n",
    "        similar_words = w2v_model.wv.most_similar(word, topn=5)\n",
    "        print(f\"Most similar words to '{word}':\")\n",
    "        for similar_word, similarity in similar_words:\n",
    "            print(f\"  {similar_word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40a9c9f3-aaa9-446e-81a8-44fdfe710c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===== Problem 7: Vector Visualization =====\n",
      "Sentiment words in vocabulary: ['great', 'good', 'excellent', 'fantastic', 'terrible', 'bad', 'awful', 'horrible']\n",
      "Movie words in vocabulary: ['movie', 'film', 'cinema', 'documentary', 'scene', 'actor', 'actress', 'director']\n",
      "Rating words in vocabulary: ['star', 'rating', 'review', 'recommend', 'suggest', 'watch', 'avoid']\n",
      "Visualizing would display plots here, but in a real environment we'd see the t-SNE plots\n"
     ]
    }
   ],
   "source": [
    "# Problem 7: Vector Visualization\n",
    "print(\"\\n\\n===== Problem 7: Vector Visualization =====\")\n",
    "\n",
    "def visualize_embeddings(model, words=None, n_words=50):\n",
    "    \"\"\"\n",
    "    Visualize word embeddings using t-SNE\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : Word2Vec model\n",
    "        Trained Word2Vec model\n",
    "    words : list, optional\n",
    "        Specific words to visualize. If None, top n_words by frequency will be used\n",
    "    n_words : int, optional\n",
    "        Number of words to visualize if words is None\n",
    "    \"\"\"\n",
    "    # Get embedding matrix\n",
    "    if words is None:\n",
    "        # Get the most common words\n",
    "        words = [word for word, vocab in \n",
    "                 sorted(model.wv.key_to_index.items(), \n",
    "                        key=lambda item: model.wv.get_vecattr(item[0], \"count\"),\n",
    "                        reverse=True)[:n_words]]\n",
    "    else:\n",
    "        # Filter out words not in vocabulary\n",
    "        words = [word for word in words if word in model.wv]\n",
    "    \n",
    "    # Extract word vectors\n",
    "    word_vectors = np.array([model.wv[word] for word in words])\n",
    "    \n",
    "    # Apply t-SNE dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(words)-1))\n",
    "    embedded_vectors = tsne.fit_transform(word_vectors)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot all points\n",
    "    plt.scatter(embedded_vectors[:, 0], embedded_vectors[:, 1], s=10, alpha=0.5)\n",
    "    \n",
    "    # Annotate words\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, xy=(embedded_vectors[i, 0], embedded_vectors[i, 1]),\n",
    "                     xytext=(5, 2), textcoords='offset points',\n",
    "                     ha='right', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.title(\"t-SNE visualization of Word2Vec embeddings\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define interesting word groups to visualize\n",
    "sentiment_words = ['great', 'good', 'excellent', 'fantastic', 'terrible', 'bad', 'awful', 'horrible']\n",
    "movie_words = ['movie', 'film', 'cinema', 'documentary', 'scene', 'actor', 'actress', 'director']\n",
    "rating_words = ['star', 'rating', 'review', 'recommend', 'suggest', 'watch', 'avoid']\n",
    "\n",
    "# Filter to include only words in vocabulary\n",
    "sentiment_words = [word for word in sentiment_words if word in w2v_model.wv]\n",
    "movie_words = [word for word in movie_words if word in w2v_model.wv]\n",
    "rating_words = [word for word in rating_words if word in w2v_model.wv]\n",
    "\n",
    "print(f\"Sentiment words in vocabulary: {sentiment_words}\")\n",
    "print(f\"Movie words in vocabulary: {movie_words}\")\n",
    "print(f\"Rating words in vocabulary: {rating_words}\")\n",
    "\n",
    "print(\"Visualizing would display plots here, but in a real environment we'd see the t-SNE plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63ddac17-4cb9-4679-a410-dc2830ab3f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===== Problem 8: Movie review classification using Word2Vec =====\n",
      "Vectorizing documents...\n",
      "Vectorization completed in 39.76 seconds\n",
      "\n",
      "Training logistic regression on Word2Vec vectors...\n",
      "Training time: 0.62 seconds\n",
      "Prediction time: 0.01 seconds\n",
      "Accuracy: 0.8535\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.86     12500\n",
      "           1       0.86      0.84      0.85     12500\n",
      "\n",
      "    accuracy                           0.85     25000\n",
      "   macro avg       0.85      0.85      0.85     25000\n",
      "weighted avg       0.85      0.85      0.85     25000\n",
      "\n",
      "\n",
      "Comparison of methods:\n",
      "1. TF-IDF (baseline): 0.8810\n",
      "2. Word2Vec average vectors: 0.8535\n"
     ]
    }
   ],
   "source": [
    "# Problem 8: Movie review classification using Word2Vec\n",
    "print(\"\\n\\n===== Problem 8: Movie review classification using Word2Vec =====\")\n",
    "\n",
    "def create_document_vector(tokens, word_vectors, vector_size=100):\n",
    "    \"\"\"\n",
    "    Create a document vector by averaging word vectors\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    tokens : list\n",
    "        List of words in the document\n",
    "    word_vectors : Word2Vec.wv\n",
    "        Word vectors from trained Word2Vec model\n",
    "    vector_size : int\n",
    "        Dimensionality of word vectors\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Document vector of shape (vector_size,)\n",
    "    \"\"\"\n",
    "    # Initialize document vector\n",
    "    doc_vector = np.zeros(vector_size)\n",
    "    \n",
    "    # Count words with vectors\n",
    "    word_count = 0\n",
    "    \n",
    "    # Sum all word vectors\n",
    "    for token in tokens:\n",
    "        if token in word_vectors:\n",
    "            doc_vector += word_vectors[token]\n",
    "            word_count += 1\n",
    "    \n",
    "    # Average the vectors\n",
    "    if word_count > 0:\n",
    "        doc_vector /= word_count\n",
    "    \n",
    "    return doc_vector\n",
    "\n",
    "def vectorize_documents(documents, word_vectors, vector_size=100):\n",
    "    \"\"\"\n",
    "    Vectorize a list of documents using Word2Vec\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    documents : list\n",
    "        List of tokenized documents\n",
    "    word_vectors : Word2Vec.wv\n",
    "        Word vectors from trained Word2Vec model\n",
    "    vector_size : int\n",
    "        Dimensionality of word vectors\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Document vectors of shape (n_documents, vector_size)\n",
    "    \"\"\"\n",
    "    # Initialize matrix for all document vectors\n",
    "    doc_vectors = np.zeros((len(documents), vector_size))\n",
    "    \n",
    "    # Process each document\n",
    "    for i, tokens in enumerate(documents):\n",
    "        doc_vectors[i] = create_document_vector(tokens, word_vectors, vector_size)\n",
    "        \n",
    "    return doc_vectors\n",
    "\n",
    "# Vectorize documents using our trained Word2Vec model\n",
    "print(\"Vectorizing documents...\")\n",
    "start_time = time.time()\n",
    "X_train_w2v = vectorize_documents(preprocessed_train, w2v_model.wv, vector_size)\n",
    "X_test_w2v = vectorize_documents(preprocessed_test, w2v_model.wv, vector_size)\n",
    "vectorize_time = time.time() - start_time\n",
    "print(f\"Vectorization completed in {vectorize_time:.2f} seconds\")\n",
    "\n",
    "# Train a logistic regression model on Word2Vec vectors\n",
    "print(\"\\nTraining logistic regression on Word2Vec vectors...\")\n",
    "classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "start_time = time.time()\n",
    "classifier.fit(X_train_w2v, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Predict\n",
    "start_time = time.time()\n",
    "y_pred = classifier.predict(X_test_w2v)\n",
    "predict_time = time.time() - start_time\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Training time: {train_time:.2f} seconds\")\n",
    "print(f\"Prediction time: {predict_time:.2f} seconds\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compare Word2Vec with TF-IDF\n",
    "print(\"\\nComparison of methods:\")\n",
    "print(f\"1. TF-IDF (baseline): {baseline_accuracy:.4f}\")\n",
    "print(f\"2. Word2Vec average vectors: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be30b347-9b75-4c77-ab22-cf571cc7e85e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b892016-4f02-4933-b60f-391c47a92a80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
