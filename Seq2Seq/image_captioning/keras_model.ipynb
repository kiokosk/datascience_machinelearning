{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fcc8504-d38f-4348-90cb-aa6651b862b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Summary:\n",
      "Model: \"encoder_cnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet152 (Functional)      (None, 2048)              58370944  \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  524544    \n",
      "                                                                 \n",
      " batch_normalization (Batch  multiple                  1024      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58896512 (224.67 MB)\n",
      "Trainable params: 525056 (2.00 MB)\n",
      "Non-trainable params: 58371456 (222.67 MB)\n",
      "_________________________________________________________________\n",
      "\n",
      "Decoder Summary:\n",
      "Model: \"decoder_rnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  2560000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 multiple                  1574912   \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  5130000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9264912 (35.34 MB)\n",
      "Trainable params: 9264912 (35.34 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Example Inference:\n",
      "Feature shape: (1, 256)\n",
      "Generated caption ids shape: (1, 20)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet152\n",
    "import numpy as np\n",
    "\n",
    "class EncoderCNN(models.Model):\n",
    "    \"\"\"\n",
    "    Keras implementation of the EncoderCNN from the PyTorch model.py\n",
    "    This encoder uses a pretrained ResNet-152 to extract features from images.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "          \n",
    "        Args:\n",
    "            embed_size: dimension of the feature vectors\n",
    "        \"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet but exclude the final FC layer\n",
    "        base_model = ResNet152(include_top=False, weights='imagenet', pooling='avg')\n",
    "        \n",
    "        # Freeze the ResNet layers\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "            \n",
    "        self.resnet = base_model\n",
    "        self.linear = layers.Dense(embed_size)\n",
    "        self.bn = layers.BatchNormalization(momentum=0.01)\n",
    "        \n",
    "    def call(self, images, training=False):\n",
    "        \"\"\"\n",
    "        Extract feature vectors from input images.\n",
    "        \n",
    "        Args:\n",
    "            images: input images, (batch_size, height, width, channels)\n",
    "            training: whether the call is in training mode\n",
    "            \n",
    "        Returns:\n",
    "            features: feature vectors, (batch_size, embed_size)\n",
    "        \"\"\"\n",
    "        features = self.resnet(images)\n",
    "        features = self.linear(features)\n",
    "        features = self.bn(features, training=training)\n",
    "        return features\n",
    "\n",
    "\n",
    "class DecoderRNN(models.Model):\n",
    "    \"\"\"\n",
    "    Keras implementation of the DecoderRNN from the PyTorch model.py\n",
    "    This decoder takes image features and generates captions.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, max_seq_length=20):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \n",
    "        Args:\n",
    "            embed_size: dimension of word embeddings\n",
    "            hidden_size: dimension of LSTM hidden states\n",
    "            vocab_size: size of vocabulary\n",
    "            num_layers: number of LSTM layers\n",
    "            max_seq_length: maximum sequence length for generation\n",
    "        \"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.embed = layers.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # Create stacked LSTM\n",
    "        if num_layers == 1:\n",
    "            self.lstm = layers.LSTM(hidden_size, return_sequences=True, return_state=True)\n",
    "        else:\n",
    "            # For multi-layer LSTM\n",
    "            lstm_cells = [layers.LSTMCell(hidden_size) for _ in range(num_layers)]\n",
    "            self.lstm = layers.RNN(lstm_cells, return_sequences=True, return_state=True)\n",
    "            \n",
    "        self.linear = layers.Dense(vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def call(self, features, captions, lengths=None, training=False):\n",
    "        \"\"\"\n",
    "        Decode image feature vectors and generates captions.\n",
    "        \n",
    "        Args:\n",
    "            features: image features, (batch_size, embed_size)\n",
    "            captions: encoded captions, (batch_size, max_caption_length)\n",
    "            lengths: valid lengths for each caption\n",
    "            training: whether the call is in training mode\n",
    "            \n",
    "        Returns:\n",
    "            outputs: predicted scores for each word in vocabulary\n",
    "        \"\"\"\n",
    "        # Embed word indices to vectors\n",
    "        embeddings = self.embed(captions)\n",
    "        \n",
    "        # Prepare features for input to LSTM\n",
    "        features_expanded = tf.expand_dims(features, 1)\n",
    "        \n",
    "        # Concatenate features with embeddings\n",
    "        # In TF, we need to handle padding differently than PyTorch's pack_padded_sequence\n",
    "        inputs = tf.concat([features_expanded, embeddings[:, :-1]], axis=1)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        hidden_states, *_ = self.lstm(inputs, training=training)\n",
    "        \n",
    "        # Predict next words\n",
    "        outputs = self.linear(hidden_states)\n",
    "        \n",
    "        # If lengths are provided, we should mask the outputs\n",
    "        if lengths is not None:\n",
    "            # Create a mask based on lengths\n",
    "            mask = tf.sequence_mask(lengths, tf.shape(outputs)[1], dtype=tf.float32)\n",
    "            # Apply mask\n",
    "            outputs = outputs * tf.expand_dims(mask, -1)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, features):\n",
    "        \"\"\"\n",
    "        Generate captions for given image features using greedy search.\n",
    "        \n",
    "        Args:\n",
    "            features: image features, (batch_size, embed_size)\n",
    "            \n",
    "        Returns:\n",
    "            sampled_ids: sampled word ids, (batch_size, max_seq_length)\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(features)[0]\n",
    "        sampled_ids = []\n",
    "        \n",
    "        # Initial state and input\n",
    "        states = None\n",
    "        inputs = tf.expand_dims(features, 1)\n",
    "        \n",
    "        for i in range(self.max_seq_length):\n",
    "            # First time step or not\n",
    "            if states is None:\n",
    "                outputs, h, c = self.lstm(inputs, training=False)\n",
    "                states = [h, c]\n",
    "            else:\n",
    "                outputs, h, c = self.lstm(inputs, initial_state=states, training=False)\n",
    "                states = [h, c]\n",
    "            \n",
    "            # Predict the next word\n",
    "            outputs = self.linear(outputs)\n",
    "            predicted = tf.argmax(outputs, axis=2)\n",
    "            \n",
    "            # Save the predicted word id\n",
    "            sampled_ids.append(predicted[:, 0])\n",
    "            \n",
    "            # Prepare input for the next step\n",
    "            inputs = self.embed(predicted)\n",
    "        \n",
    "        # Stack the sampled ids to a tensor and return\n",
    "        sampled_ids = tf.stack(sampled_ids, axis=1)\n",
    "        return sampled_ids\n",
    "\n",
    "\n",
    "# Function to create the complete image captioning model\n",
    "def create_image_captioning_model(embed_size, hidden_size, vocab_size, num_layers=1, max_seq_length=20):\n",
    "    \"\"\"\n",
    "    Create the complete image captioning model with encoder and decoder.\n",
    "    \n",
    "    Args:\n",
    "        embed_size: dimension of word embeddings\n",
    "        hidden_size: dimension of LSTM hidden states\n",
    "        vocab_size: size of vocabulary\n",
    "        num_layers: number of LSTM layers\n",
    "        max_seq_length: maximum sequence length for generation\n",
    "        \n",
    "    Returns:\n",
    "        encoder: EncoderCNN model\n",
    "        decoder: DecoderRNN model\n",
    "    \"\"\"\n",
    "    # Create encoder and decoder\n",
    "    encoder = EncoderCNN(embed_size)\n",
    "    decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers, max_seq_length)\n",
    "    \n",
    "    # Build models by calling them with sample data\n",
    "    sample_image = tf.random.normal((1, 224, 224, 3))\n",
    "    sample_caption = tf.zeros((1, 10), dtype=tf.int32)\n",
    "    \n",
    "    features = encoder(sample_image)\n",
    "    _ = decoder(features, sample_caption)\n",
    "    \n",
    "    return encoder, decoder\n",
    "\n",
    "\n",
    "# Function to convert PyTorch model weights to Keras\n",
    "def convert_pytorch_to_keras(pytorch_encoder_path, pytorch_decoder_path, keras_encoder, keras_decoder):\n",
    "    \"\"\"\n",
    "    Convert PyTorch model weights to Keras format.\n",
    "    \n",
    "    Args:\n",
    "        pytorch_encoder_path: path to the PyTorch encoder weights\n",
    "        pytorch_decoder_path: path to the PyTorch decoder weights\n",
    "        keras_encoder: Keras encoder model\n",
    "        keras_decoder: Keras decoder model\n",
    "        \n",
    "    Returns:\n",
    "        keras_encoder: Keras encoder with loaded weights\n",
    "        keras_decoder: Keras decoder with loaded weights\n",
    "    \"\"\"\n",
    "    import torch  \n",
    "    \n",
    "    # Load PyTorch model weights\n",
    "    encoder_state_dict = torch.load(pytorch_encoder_path, map_location=torch.device('cpu'))\n",
    "    decoder_state_dict = torch.load(pytorch_decoder_path, map_location=torch.device('cpu'))\n",
    "    \n",
    "    # Convert encoder weights\n",
    "    # ResNet weights are already loaded from ImageNet\n",
    "    \n",
    "    # Convert linear layer weights\n",
    "    keras_encoder.linear.set_weights([\n",
    "        encoder_state_dict['linear.weight'].numpy().T,  # Transpose for Keras format\n",
    "        encoder_state_dict['linear.bias'].numpy()\n",
    "    ])\n",
    "    \n",
    "    # Convert batch norm weights\n",
    "    keras_encoder.bn.set_weights([\n",
    "        encoder_state_dict['bn.weight'].numpy(),        # gamma\n",
    "        encoder_state_dict['bn.bias'].numpy(),          # beta\n",
    "        encoder_state_dict['bn.running_mean'].numpy(),  # running mean\n",
    "        encoder_state_dict['bn.running_var'].numpy()    # running variance\n",
    "    ])\n",
    "    \n",
    "    # Convert decoder weights\n",
    "    # Embedding layer\n",
    "    keras_decoder.embed.set_weights([\n",
    "        decoder_state_dict['embed.weight'].numpy()\n",
    "    ])\n",
    "    \n",
    "\n",
    "    if keras_decoder.num_layers == 1:\n",
    "        # For single layer LSTM\n",
    "        # Extract PyTorch weights\n",
    "        w_ih = decoder_state_dict['lstm.weight_ih_l0'].numpy()\n",
    "        w_hh = decoder_state_dict['lstm.weight_hh_l0'].numpy()\n",
    "        b_ih = decoder_state_dict['lstm.bias_ih_l0'].numpy()\n",
    "        b_hh = decoder_state_dict['lstm.bias_hh_l0'].numpy()\n",
    "        \n",
    "        # PyTorch LSTM weights are [ifgo] format while Keras uses [iofc]\n",
    "        # Reorder and concatenate weights for Keras format\n",
    "        w_i, w_f, w_g, w_o = np.split(w_ih, 4)\n",
    "        w_hi, w_hf, w_hg, w_ho = np.split(w_hh, 4)\n",
    "        \n",
    "        b_i, b_f, b_g, b_o = np.split(b_ih, 4)\n",
    "        b_hi, b_hf, b_hg, b_ho = np.split(b_hh, 4)\n",
    "        \n",
    "        # Reorder to [i, o, f, g] for Keras\n",
    "        # Concat input and recurrent weights\n",
    "        keras_w = np.concatenate([\n",
    "            np.concatenate([w_i, w_o, w_f, w_g], axis=0),\n",
    "            np.concatenate([w_hi, w_ho, w_hf, w_hg], axis=0)\n",
    "        ], axis=1)\n",
    "        \n",
    "        # Concat input and recurrent biases\n",
    "        keras_b = np.concatenate([\n",
    "            b_i + b_hi,\n",
    "            b_o + b_ho,\n",
    "            b_f + b_hf,\n",
    "            b_g + b_hg\n",
    "        ])\n",
    "        \n",
    "        keras_decoder.lstm.set_weights([keras_w, keras_b])\n",
    "    \n",
    "    # Linear layer\n",
    "    keras_decoder.linear.set_weights([\n",
    "        decoder_state_dict['linear.weight'].numpy().T,\n",
    "        decoder_state_dict['linear.bias'].numpy()\n",
    "    ])\n",
    "    \n",
    "    return keras_encoder, keras_decoder\n",
    "\n",
    "\n",
    "# Sample usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create models\n",
    "    embed_size = 256\n",
    "    hidden_size = 512\n",
    "    vocab_size = 10000  # Example size\n",
    "    \n",
    "    encoder, decoder = create_image_captioning_model(\n",
    "        embed_size=embed_size,\n",
    "        hidden_size=hidden_size,\n",
    "        vocab_size=vocab_size\n",
    "    )\n",
    "    \n",
    "    # Print model summaries\n",
    "    print(\"Encoder Summary:\")\n",
    "    encoder.summary()\n",
    "    \n",
    "    print(\"\\nDecoder Summary:\")\n",
    "    decoder.summary()\n",
    "    \n",
    "    # Example of converting weights (would require PyTorch)\n",
    "    # convert_pytorch_to_keras(\n",
    "    #     pytorch_encoder_path=\"models/encoder-5-3000.pkl\",\n",
    "    #     pytorch_decoder_path=\"models/decoder-5-3000.pkl\",\n",
    "    #     keras_encoder=encoder,\n",
    "    #     keras_decoder=decoder\n",
    "    # )\n",
    "    \n",
    "    # Example of inference\n",
    "    print(\"\\nExample Inference:\")\n",
    "    sample_image = tf.random.normal((1, 224, 224, 3))\n",
    "    features = encoder(sample_image, training=False)\n",
    "    print(f\"Feature shape: {features.shape}\")\n",
    "    \n",
    "    sample_ids = decoder.sample(features)\n",
    "    print(f\"Generated caption ids shape: {sample_ids.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8929b43-a052-4864-a861-cf8d4103e260",
   "metadata": {},
   "source": [
    "## Weight Transfer Challenges\n",
    "\n",
    "Converting weights from PyTorch to Keras involves several challenges:\n",
    "\n",
    "- **Different Tensor Ordering**:  \n",
    "  PyTorch uses `[out_channels, in_channels, height, width]` for convolutional filters, while TensorFlow/Keras uses `[height, width, in_channels, out_channels]`.\n",
    "\n",
    "- **LSTM Implementation Differences**:  \n",
    "  PyTorch and Keras have different implementations of LSTM cells, so parameters need careful mapping.\n",
    "\n",
    "- **Batch Normalization**:  \n",
    "  The parameters (mean, variance, gamma, beta) need to be carefully mapped.\n",
    "\n",
    "- **Custom Script Required**:  \n",
    "  You'd need to write a custom script to:\n",
    "  - Load PyTorch weights using PyTorch\n",
    "  - Map each weight tensor to the corresponding Keras layer\n",
    "  - Apply necessary transformations (transposing, reshaping)\n",
    "  - Set the weights in the Keras model\n",
    "\n",
    "---\n",
    "\n",
    "## Advanced Topics\n",
    "\n",
    "### Translating to Other Languages\n",
    "\n",
    "To adapt the model for translation between languages (e.g., Japanese and English):\n",
    "\n",
    "- **Data Collection**:  \n",
    "  Gather a dataset of image-caption pairs in both languages.\n",
    "\n",
    "- **Bilingual Vocabulary**:  \n",
    "  Create a vocabulary that includes words from both languages.\n",
    "\n",
    "- **Architecture Modification**:  \n",
    "  Modify the decoder to handle multiple languages:\n",
    "  - Add language identifier tokens\n",
    "  - Increase the vocabulary size to accommodate both languages\n",
    "  - Train the model on mixed-language data\n",
    "\n",
    "- **Specialized Approach**: For better results, you could:\n",
    "  - Train an image captioning model for the source language\n",
    "  - Use a neural machine translation model to translate the generated captions\n",
    "\n",
    "---\n",
    "\n",
    "### Advanced Methods of Machine Translation\n",
    "\n",
    "Modern machine translation has evolved significantly:\n",
    "\n",
    "- **Attention Mechanisms**:  \n",
    "  Allow the model to focus on different parts of the input when generating each word of the output:\n",
    "  - *Bahdanau Attention*: Introduced in _\"Neural Machine Translation by Jointly Learning to Align and Translate\"_\n",
    "  - *Luong Attention*: A simpler, more computationally efficient approach\n",
    "\n",
    "- **Transformer Models**: Revolutionized machine translation:\n",
    "  - Based on self-attention rather than RNNs\n",
    "  - Enables parallel processing (unlike sequential RNNs)\n",
    "  - Examples: Google's **T5**, Facebook's **MBART**, OpenAI's **GPT** series\n",
    "\n",
    "- **Multimodal Translation**: Incorporating visual and textual information:\n",
    "  - Visual-grounded translation that uses images alongside text\n",
    "  - Particularly useful for ambiguous text\n",
    "\n",
    "- **Unsupervised Translation**: Using monolingual corpora to learn translation:\n",
    "  - Useful for low-resource language pairs\n",
    "  - Based on techniques like back-translation and denoising auto-encoders\n",
    "\n",
    "---\n",
    "\n",
    "## Generating Images from Text\n",
    "\n",
    "Text-to-image generation has made remarkable progress:\n",
    "\n",
    "### GAN-Based Approaches:\n",
    "\n",
    "- **StackGAN**: Generates images in stages, increasing resolution  \n",
    "- **AttnGAN**: Uses attention mechanisms between text and image features\n",
    "\n",
    "### Diffusion Models:\n",
    "\n",
    "- **DALL-E (OpenAI)**: Generates images from text descriptions using a VQ-VAE and transformer  \n",
    "- **Stable Diffusion**: Open-source diffusion model for high-quality image generation  \n",
    "- **Midjourney**: Produces highly artistic renderings from text prompts\n",
    "\n",
    "### Latent Variable Models:\n",
    "\n",
    "- **VQ-GAN**: Combines vector quantization with GANs for improved image quality  \n",
    "- **CLIP-guided synthesis**: Uses CLIP to align text embeddings with generated images\n",
    "\n",
    "### Architecture Components:\n",
    "\n",
    "- **Text encoders**: Transform text into embeddings  \n",
    "- **Cross-attention**: Connect text features to visual features  \n",
    "- **Image decoders**: Generate images from conditioned latent representations\n",
    "\n",
    "### Training Methods:\n",
    "\n",
    "- Contrastive learning between text and images  \n",
    "- Adversarial training for realism  \n",
    "- Classifier guidance for semantic alignment\n",
    "\n",
    "> These text-to-image systems are the conceptual opposite of image captioning, translating from the linguistic domain to the visual domain instead of the other way around.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc86a90-186b-466c-be07-689d63121f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
