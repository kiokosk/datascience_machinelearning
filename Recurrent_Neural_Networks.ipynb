{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b41f9ecc-1ac8-4f1b-8268-ecfbf4660744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment with small arrays of forward propagation:\n",
      "Expected output from document:\n",
      "h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])\n",
      "\n",
      "Actual calculated output:\n",
      "h = np.array([[0.7949422790422093, 0.8183900239382846, 0.8393964886275626, 0.8558417381114936]])\n",
      "\n",
      "The output matches the expected values!\n",
      "\n",
      "Using SimpleRNN class:\n",
      "SimpleRNN output: [[0.79494228 0.81839002 0.83939649 0.85584174]]\n",
      "The SimpleRNN output matches the expected values!\n",
      "\n",
      "Example of ScratchSimpleRNNClassifier:\n",
      "Sample predictions: [2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleRNN:\n",
    "    def __init__(self, n_features, n_nodes):\n",
    "        # Initialize weights and biases\n",
    "        self.W_x = np.random.randn(n_features, n_nodes) * 0.01  # Input weights\n",
    "        self.W_h = np.random.randn(n_nodes, n_nodes) * 0.01     # Hidden state weights\n",
    "        self.b = np.zeros(n_nodes)                             # Bias term\n",
    "        \n",
    "        # For storing values during forward pass (needed for backprop)\n",
    "        self.cache = None\n",
    "        self.dW_x = None  # Gradients for input weights\n",
    "        self.dW_h = None  # Gradients for hidden state weights\n",
    "        self.db = None    # Gradients for bias\n",
    "        \n",
    "    def forward(self, x, h_prev=None):\n",
    "        \"\"\"\n",
    "        Forward pass of SimpleRNN\n",
    "        \n",
    "        Parameters:\n",
    "            x: Input data with shape (batch_size, n_sequences, n_features)\n",
    "            h_prev: Initial hidden state (if None, initialize with zeros)\n",
    "            \n",
    "        Returns:\n",
    "            h: Final hidden state after processing all sequences\n",
    "        \"\"\"\n",
    "        batch_size, n_sequences, n_features = x.shape\n",
    "        n_nodes = self.W_x.shape[1]\n",
    "        \n",
    "        # Initialize the hidden state if not provided\n",
    "        if h_prev is None:\n",
    "            h_prev = np.zeros((batch_size, n_nodes))\n",
    "        \n",
    "        # Store inputs, hidden states, and activations for backpropagation\n",
    "        xs, hs, hs_prev, as_ = {}, {}, {}, {}\n",
    "        hs[-1] = h_prev\n",
    "        h = h_prev\n",
    "        \n",
    "        # Process each time step\n",
    "        for t in range(n_sequences):\n",
    "            # Get current input\n",
    "            xs[t] = x[:, t, :]\n",
    "            hs_prev[t] = h\n",
    "            \n",
    "            # Calculate pre-activation\n",
    "            a = np.dot(xs[t], self.W_x) + np.dot(h, self.W_h) + self.b\n",
    "            as_[t] = a\n",
    "            \n",
    "            # Apply activation function (tanh)\n",
    "            h = np.tanh(a)\n",
    "            hs[t] = h\n",
    "        \n",
    "        # Store values for backpropagation\n",
    "        self.cache = (xs, hs, hs_prev, as_)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    def backward(self, dh, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Backward pass of SimpleRNN\n",
    "        \n",
    "        Parameters:\n",
    "            dh: Gradient of loss with respect to the final hidden state\n",
    "            learning_rate: Learning rate for weight updates\n",
    "            \n",
    "        Returns:\n",
    "            dx: Gradient of loss with respect to input x\n",
    "            dh_prev: Gradient of loss with respect to initial hidden state\n",
    "        \"\"\"\n",
    "        xs, hs, hs_prev, as_ = self.cache\n",
    "        \n",
    "        batch_size, n_nodes = dh.shape\n",
    "        n_sequences = len(xs)\n",
    "        n_features = xs[0].shape[1]\n",
    "        \n",
    "        # Initialize gradients\n",
    "        dW_x = np.zeros_like(self.W_x)\n",
    "        dW_h = np.zeros_like(self.W_h)\n",
    "        db = np.zeros_like(self.b)\n",
    "        dh_prev = np.zeros_like(dh)\n",
    "        dx = np.zeros((batch_size, n_sequences, n_features))\n",
    "        \n",
    "        # Backpropagate through time\n",
    "        for t in reversed(range(n_sequences)):\n",
    "            # Gradient of loss with respect to hidden state\n",
    "            if t == n_sequences - 1:\n",
    "                dh_next = dh\n",
    "            else:\n",
    "                dh_next = dh_next + dh_t\n",
    "            \n",
    "            # Gradient of loss with respect to pre-activation\n",
    "            da = dh_next * (1 - hs[t]**2)  # Derivative of tanh\n",
    "            \n",
    "            # Gradients of loss with respect to parameters\n",
    "            db += np.sum(da, axis=0)\n",
    "            dW_x += np.dot(xs[t].T, da)\n",
    "            dW_h += np.dot(hs_prev[t].T, da)\n",
    "            \n",
    "            # Gradient of loss with respect to previous hidden state\n",
    "            dh_t = np.dot(da, self.W_h.T)\n",
    "            \n",
    "            # Gradient of loss with respect to input\n",
    "            dx[:, t, :] = np.dot(da, self.W_x.T)\n",
    "        \n",
    "        # Update parameters\n",
    "        self.W_x -= learning_rate * dW_x\n",
    "        self.W_h -= learning_rate * dW_h\n",
    "        self.b -= learning_rate * db\n",
    "        \n",
    "        # Store gradients\n",
    "        self.dW_x = dW_x\n",
    "        self.dW_h = dW_h\n",
    "        self.db = db\n",
    "        \n",
    "        return dx, dh_prev\n",
    "\n",
    "class FullyConnected:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.W = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.b = np.zeros(output_size)\n",
    "        \n",
    "        # For storing values during forward pass (needed for backprop)\n",
    "        self.cache = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of fully connected layer\n",
    "        \n",
    "        Parameters:\n",
    "            x: Input data with shape (batch_size, input_size)\n",
    "            \n",
    "        Returns:\n",
    "            out: Output of the fully connected layer\n",
    "        \"\"\"\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        self.cache = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Backward pass of fully connected layer\n",
    "        \n",
    "        Parameters:\n",
    "            dout: Gradient of loss with respect to output\n",
    "            learning_rate: Learning rate for weight updates\n",
    "            \n",
    "        Returns:\n",
    "            dx: Gradient of loss with respect to input x\n",
    "        \"\"\"\n",
    "        x = self.cache\n",
    "        \n",
    "        # Compute gradients\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        dW = np.dot(x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        \n",
    "        # Update parameters\n",
    "        self.W -= learning_rate * dW\n",
    "        self.b -= learning_rate * db\n",
    "        \n",
    "        # Store gradients\n",
    "        self.dW = dW\n",
    "        self.db = db\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class ScratchSimpleRNNClassifier:\n",
    "    def __init__(self, n_features, n_hidden, n_classes, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize ScratchSimpleRNNClassifier\n",
    "        \n",
    "        Parameters:\n",
    "            n_features: Number of input features\n",
    "            n_hidden: Number of hidden nodes in RNN\n",
    "            n_classes: Number of output classes\n",
    "            learning_rate: Learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        self.rnn = SimpleRNN(n_features, n_hidden)\n",
    "        self.fc = FullyConnected(n_hidden, n_classes)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        \n",
    "        Parameters:\n",
    "            x: Input data with shape (batch_size, n_sequences, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            scores: Raw output scores before softmax\n",
    "        \"\"\"\n",
    "        # Pass through RNN\n",
    "        h = self.rnn.forward(x)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        scores = self.fc.forward(h)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict class labels for input data\n",
    "        \n",
    "        Parameters:\n",
    "            x: Input data with shape (batch_size, n_sequences, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            y_pred: Predicted class labels\n",
    "        \"\"\"\n",
    "        scores = self.forward(x)\n",
    "        y_pred = np.argmax(scores, axis=1)\n",
    "        return y_pred\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Compute softmax values for each set of scores\n",
    "        \n",
    "        Parameters:\n",
    "            x: Input scores\n",
    "            \n",
    "        Returns:\n",
    "            softmax_output: Softmax probabilities\n",
    "        \"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        \"\"\"\n",
    "        Compute cross-entropy loss and gradients\n",
    "        \n",
    "        Parameters:\n",
    "            x: Input data with shape (batch_size, n_sequences, n_features)\n",
    "            y: True class labels\n",
    "            \n",
    "        Returns:\n",
    "            loss: Cross-entropy loss\n",
    "            dscores: Gradient of loss with respect to scores\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Forward pass\n",
    "        scores = self.forward(x)\n",
    "        probs = self.softmax(scores)\n",
    "        \n",
    "        # Compute cross-entropy loss\n",
    "        loss = -np.sum(np.log(probs[np.arange(batch_size), y])) / batch_size\n",
    "        \n",
    "        # Compute gradients\n",
    "        dscores = probs.copy()\n",
    "        dscores[np.arange(batch_size), y] -= 1\n",
    "        dscores /= batch_size\n",
    "        \n",
    "        return loss, dscores\n",
    "    \n",
    "    def backward(self, x, y):\n",
    "        \"\"\"\n",
    "        Backward pass through the network\n",
    "        \n",
    "        Parameters:\n",
    "            x: Input data with shape (batch_size, n_sequences, n_features)\n",
    "            y: True class labels\n",
    "            \n",
    "        Returns:\n",
    "            loss: Cross-entropy loss\n",
    "        \"\"\"\n",
    "        # Compute loss and gradients\n",
    "        loss, dscores = self.loss(x, y)\n",
    "        \n",
    "        # Backpropagate through fully connected layer\n",
    "        dh = self.fc.backward(dscores, self.learning_rate)\n",
    "        \n",
    "        # Backpropagate through RNN\n",
    "        dx, _ = self.rnn.backward(dh, self.learning_rate)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X, y, epochs=100, batch_size=32, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \n",
    "        Parameters:\n",
    "            X: Input data with shape (n_samples, n_sequences, n_features)\n",
    "            y: Target labels\n",
    "            epochs: Number of training epochs\n",
    "            batch_size: Size of mini-batches\n",
    "            verbose: Whether to print training progress\n",
    "            \n",
    "        Returns:\n",
    "            losses: List of training losses\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            # Mini-batch training\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                # Backward pass (updates weights)\n",
    "                loss = self.backward(X_batch, y_batch)\n",
    "                \n",
    "            losses.append(loss)\n",
    "            \n",
    "            if verbose and (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "\n",
    "# Test with the small example\n",
    "if __name__ == \"__main__\":\n",
    "    # Test data exactly as specified\n",
    "    x = np.array([[[1, 2], [2, 3], [3, 4]]])/100  # (batch_size, n_sequences, n_features)\n",
    "    w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100  # (n_features, n_nodes)\n",
    "    w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100  # (n_nodes, n_nodes)\n",
    "    batch_size = x.shape[0]  # 1\n",
    "    n_sequences = x.shape[1]  # 3\n",
    "    n_features = x.shape[2]  # 2\n",
    "    n_nodes = w_x.shape[1]  # 4\n",
    "    h = np.zeros((batch_size, n_nodes))  # (batch_size, n_nodes)\n",
    "    b = np.array([1, 1, 1, 1])  # (n_nodes,)\n",
    "    \n",
    "    # Experiment with small arrays of forward propagation\n",
    "    print(\"Experiment with small arrays of forward propagation:\")\n",
    "    \n",
    "    # Time step 1\n",
    "    x_1 = x[:, 0, :]  # (1, 2)\n",
    "    a_1 = np.dot(x_1, w_x) + np.dot(h, w_h) + b\n",
    "    h_1 = np.tanh(a_1)\n",
    "    \n",
    "    # Time step 2\n",
    "    x_2 = x[:, 1, :]  # (1, 2)\n",
    "    a_2 = np.dot(x_2, w_x) + np.dot(h_1, w_h) + b\n",
    "    h_2 = np.tanh(a_2)\n",
    "    \n",
    "    # Time step 3\n",
    "    x_3 = x[:, 2, :]  # (1, 2)\n",
    "    a_3 = np.dot(x_3, w_x) + np.dot(h_2, w_h) + b\n",
    "    h_3 = np.tanh(a_3)\n",
    "    \n",
    "    # Final hidden state\n",
    "    h_final = h_3\n",
    "    \n",
    "    print(\"Expected output from document:\")\n",
    "    print(\"h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])\")\n",
    "    print(\"\\nActual calculated output:\")\n",
    "    print(f\"h = np.array({h_final.tolist()})\")\n",
    "    \n",
    "    # Check if the values are close to expected\n",
    "    expected = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])\n",
    "    if np.allclose(h_final, expected, rtol=1e-5, atol=1e-5):\n",
    "        print(\"\\nThe output matches the expected values!\")\n",
    "    else:\n",
    "        print(\"\\nThe output doesn't match the expected values.\")\n",
    "        print(\"Difference:\", h_final - expected)\n",
    "    \n",
    "    # Create SimpleRNN instance and manually set weights\n",
    "    print(\"\\nUsing SimpleRNN class:\")\n",
    "    rnn = SimpleRNN(n_features, n_nodes)\n",
    "    rnn.W_x = w_x.copy()\n",
    "    rnn.W_h = w_h.copy()\n",
    "    rnn.b = b.copy()  # Using the original bias without dividing by 100\n",
    "    \n",
    "    # Forward propagation\n",
    "    output = rnn.forward(x)\n",
    "    print(f\"SimpleRNN output: {output}\")\n",
    "    if np.allclose(output, expected, rtol=1e-5, atol=1e-5):\n",
    "        print(\"The SimpleRNN output matches the expected values!\")\n",
    "    else:\n",
    "        print(\"The SimpleRNN output doesn't match the expected values.\")\n",
    "        print(\"Difference:\", output - expected)\n",
    "    \n",
    "    # Example of using ScratchSimpleRNNClassifier\n",
    "    print(\"\\nExample of ScratchSimpleRNNClassifier:\")\n",
    "    # Sample data\n",
    "    X_sample = np.random.randn(10, 5, 2)  # 10 samples, 5 time steps, 2 features\n",
    "    y_sample = np.random.randint(0, 3, 10)  # 10 samples, 3 classes\n",
    "    \n",
    "    # Create and train model\n",
    "    model = ScratchSimpleRNNClassifier(n_features=2, n_hidden=4, n_classes=3, learning_rate=0.01)\n",
    "    losses = model.fit(X_sample, y_sample, epochs=50, verbose=False)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_sample)\n",
    "    print(\"Sample predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef594d6-a7ec-4c4d-8aa0-995c70401f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
