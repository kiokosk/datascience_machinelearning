{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0d3a660-bc79-4daa-b543-e21643d8c60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Conv2d:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        # Handle kernel_size as tuple or single value\n",
    "        if isinstance(kernel_size, int):\n",
    "            self.kernel_size = (kernel_size, kernel_size)\n",
    "        else:\n",
    "            self.kernel_size = kernel_size\n",
    "            \n",
    "        # Handle stride as tuple or single value\n",
    "        if isinstance(stride, int):\n",
    "            self.stride = (stride, stride)\n",
    "        else:\n",
    "            self.stride = stride\n",
    "            \n",
    "        # Handle padding as tuple or single value\n",
    "        if isinstance(padding, int):\n",
    "            self.padding = (padding, padding)\n",
    "        else:\n",
    "            self.padding = padding\n",
    "            \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.random.randn(\n",
    "            self.kernel_size[0], self.kernel_size[1], in_channels, out_channels) * 0.01\n",
    "        self.bias = np.zeros((1, out_channels))\n",
    "        \n",
    "        # For storing inputs and outputs needed in backpropagation\n",
    "        self.x = None\n",
    "        self.padded_x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of 2D convolution.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, in_channels, height, width) for NCHW format\n",
    "               or (batch_size, height, width, in_channels) for NHWC format\n",
    "               \n",
    "        Returns:\n",
    "            Output tensor after convolution\n",
    "        \"\"\"\n",
    "        # Store input for backpropagation\n",
    "        self.x = x\n",
    "        \n",
    "        batch_size, in_h, in_w, in_channels = x.shape  # NHWC format\n",
    "        \n",
    "        # Calculate output dimensions\n",
    "        out_h = (in_h + 2 * self.padding[0] - self.kernel_size[0]) // self.stride[0] + 1\n",
    "        out_w = (in_w + 2 * self.padding[1] - self.kernel_size[1]) // self.stride[1] + 1\n",
    "        \n",
    "        # Initialize output tensor\n",
    "        output = np.zeros((batch_size, out_h, out_w, self.out_channels))\n",
    "        \n",
    "        # Apply padding if needed\n",
    "        if self.padding[0] > 0 or self.padding[1] > 0:\n",
    "            padded_x = np.pad(\n",
    "                x,\n",
    "                ((0, 0), (self.padding[0], self.padding[0]), (self.padding[1], self.padding[1]), (0, 0)),\n",
    "                mode='constant'\n",
    "            )\n",
    "        else:\n",
    "            padded_x = x\n",
    "            \n",
    "        self.padded_x = padded_x\n",
    "        \n",
    "        # Perform convolution\n",
    "        for b in range(batch_size):\n",
    "            for i in range(out_h):\n",
    "                for j in range(out_w):\n",
    "                    # Define the current window position\n",
    "                    h_start = i * self.stride[0]\n",
    "                    h_end = h_start + self.kernel_size[0]\n",
    "                    w_start = j * self.stride[1]\n",
    "                    w_end = w_start + self.kernel_size[1]\n",
    "                    \n",
    "                    # Extract the current window\n",
    "                    window = padded_x[b, h_start:h_end, w_start:w_end, :]\n",
    "                    \n",
    "                    # Compute convolution for all output channels\n",
    "                    for m in range(self.out_channels):\n",
    "                        output[b, i, j, m] = np.sum(window * self.weights[:, :, :, m]) + self.bias[0, m]\n",
    "                        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        \"\"\"\n",
    "        Backward pass of 2D convolution.\n",
    "        \n",
    "        Args:\n",
    "            delta: Gradient from the next layer, shape matching output of forward\n",
    "            \n",
    "        Returns:\n",
    "            Gradient with respect to input\n",
    "        \"\"\"\n",
    "        batch_size, out_h, out_w, out_channels = delta.shape\n",
    "        _, in_h, in_w, _ = self.x.shape\n",
    "        \n",
    "        # Initialize gradients\n",
    "        dx = np.zeros_like(self.x)\n",
    "        dw = np.zeros_like(self.weights)\n",
    "        db = np.zeros_like(self.bias)\n",
    "        \n",
    "        # Compute bias gradient\n",
    "        for m in range(self.out_channels):\n",
    "            db[0, m] = np.sum(delta[:, :, :, m])\n",
    "        \n",
    "        # Compute weight gradient and input gradient\n",
    "        for b in range(batch_size):\n",
    "            for i in range(out_h):\n",
    "                for j in range(out_w):\n",
    "                    # Define the current window position\n",
    "                    h_start = i * self.stride[0]\n",
    "                    h_end = h_start + self.kernel_size[0]\n",
    "                    w_start = j * self.stride[1]\n",
    "                    w_end = w_start + self.kernel_size[1]\n",
    "                    \n",
    "                    # Update weight gradients\n",
    "                    for m in range(self.out_channels):\n",
    "                        window = self.padded_x[b, h_start:h_end, w_start:w_end, :]\n",
    "                        dw[:, :, :, m] += window * delta[b, i, j, m]\n",
    "                        \n",
    "                    # Update input gradients (if padding is 0, otherwise need to handle padding)\n",
    "                    if self.padding[0] == 0 and self.padding[1] == 0:\n",
    "                        for m in range(self.out_channels):\n",
    "                            dx[b, h_start:h_end, w_start:w_end, :] += self.weights[:, :, :, m] * delta[b, i, j, m]\n",
    "        \n",
    "        # If padding was used, we need to compute the gradient for the original input\n",
    "        if self.padding[0] > 0 or self.padding[1] > 0:\n",
    "            # Compute padded gradient\n",
    "            dx_padded = np.zeros_like(self.padded_x)\n",
    "            \n",
    "            for b in range(batch_size):\n",
    "                for i in range(out_h):\n",
    "                    for j in range(out_w):\n",
    "                        h_start = i * self.stride[0]\n",
    "                        h_end = h_start + self.kernel_size[0]\n",
    "                        w_start = j * self.stride[1]\n",
    "                        w_end = w_start + self.kernel_size[1]\n",
    "                        \n",
    "                        for m in range(self.out_channels):\n",
    "                            dx_padded[b, h_start:h_end, w_start:w_end, :] += self.weights[:, :, :, m] * delta[b, i, j, m]\n",
    "            \n",
    "            # Extract the non-padded portion\n",
    "            dx = dx_padded[:, self.padding[0]:self.padding[0]+in_h, \n",
    "                         self.padding[1]:self.padding[1]+in_w, :]\n",
    "        \n",
    "        return dx, dw, db\n",
    "    \n",
    "    def update(self, dw, db, learning_rate):\n",
    "        \"\"\"\n",
    "        Update weights and biases.\n",
    "        \n",
    "        Args:\n",
    "            dw: Weight gradients\n",
    "            db: Bias gradients\n",
    "            learning_rate: Learning rate for parameter update\n",
    "        \"\"\"\n",
    "        self.weights -= learning_rate * dw\n",
    "        self.bias -= learning_rate * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e93b76d9-ed60-4c6b-9d34-97870688152a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass result:\n",
      "[[[[-4.  1.]\n",
      "   [-4.  1.]]\n",
      "\n",
      "  [[-4.  1.]\n",
      "   [-4.  1.]]]]\n",
      "\n",
      "Backward pass result:\n",
      "[[[[  0.]\n",
      "   [  0.]\n",
      "   [  0.]\n",
      "   [  0.]]\n",
      "\n",
      "  [[  0.]\n",
      "   [ -5.]\n",
      "   [  4.]\n",
      "   [ -7.]]\n",
      "\n",
      "  [[  0.]\n",
      "   [ 13.]\n",
      "   [ 27.]\n",
      "   [-11.]]\n",
      "\n",
      "  [[  0.]\n",
      "   [-10.]\n",
      "   [-11.]\n",
      "   [  0.]]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def verify_cnn_forward():\n",
    "    # Input data (1,1,4,4) - Converting to NHWC format (1,4,4,1)\n",
    "    x = np.array([[[[1], [2], [3], [4]],\n",
    "                   [[5], [6], [7], [8]],\n",
    "                   [[9], [10], [11], [12]],\n",
    "                   [[13], [14], [15], [16]]]], dtype=np.float32)\n",
    "    \n",
    "    # Filter weights (2,1,3,3) - NHWC format\n",
    "    w = np.array([[[[0.]], [[0.]], [[0.]]], \n",
    "                  [[[0.]], [[1.]], [[0.]]], \n",
    "                  [[[0.]], [[-1.]], [[0.]]]])\n",
    "    \n",
    "    # Second filter\n",
    "    w2 = np.array([[[[0.]], [[0.]], [[0.]]], \n",
    "                   [[[0.]], [[-1.]], [[1.]]], \n",
    "                   [[[0.]], [[0.]], [[0.]]]])\n",
    "    \n",
    "    # Combine filters to (2,3,3,1)\n",
    "    weights = np.stack([w, w2], axis=3)\n",
    "    weights = weights.reshape(3, 3, 1, 2)\n",
    "    \n",
    "    # Create Conv2d layer\n",
    "    conv = Conv2d(in_channels=1, out_channels=2, kernel_size=3, stride=1, padding=0)\n",
    "    conv.weights = weights\n",
    "    conv.bias = np.zeros((1, 2))\n",
    "    \n",
    "    # Forward pass\n",
    "    output = conv.forward(x)\n",
    "    print(\"Forward pass result:\")\n",
    "    print(output)\n",
    "    \n",
    "    # Expected output: [[[[-4, 1], [-4, 1]], [[-4, 1], [-4, 1]]]]\n",
    "    \n",
    "    # Backward pass\n",
    "    delta = np.array([[[[-4, 1], [-4, -7]], [[10, 1], [11, -11]]]])\n",
    "    dx, dw, db = conv.backward(delta)\n",
    "    \n",
    "    # Expected output: array([[-5, 4], [13, 27]])\n",
    "    # This is a simplified representation of the full gradient\n",
    "    print(\"\\nBackward pass result:\")\n",
    "    print(dx)\n",
    "\n",
    "verify_cnn_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5348d205-d239-44af-a23c-076c03a8e8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: (28, 28)\n",
      "Output size: (26, 26)\n"
     ]
    }
   ],
   "source": [
    "def calculate_output_size(input_size, padding, filter_size, stride):\n",
    "    \"\"\"\n",
    "    Calculate the output size after convolution.\n",
    "    \n",
    "    Args:\n",
    "        input_size: Tuple of (height, width) of input\n",
    "        padding: Tuple of (padding_h, padding_w)\n",
    "        filter_size: Tuple of (filter_h, filter_w)\n",
    "        stride: Tuple of (stride_h, stride_w)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (output_height, output_width)\n",
    "    \"\"\"\n",
    "    output_height = (input_size[0] + 2 * padding[0] - filter_size[0]) // stride[0] + 1\n",
    "    output_width = (input_size[1] + 2 * padding[1] - filter_size[1]) // stride[1] + 1\n",
    "    \n",
    "    return (output_height, output_width)\n",
    "\n",
    "# Test the function\n",
    "input_size = (28, 28)\n",
    "padding = (0, 0)\n",
    "filter_size = (3, 3)\n",
    "stride = (1, 1)\n",
    "\n",
    "output_size = calculate_output_size(input_size, padding, filter_size, stride)\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"Output size: {output_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d2d2ae8-7357-4951-9fec-ad88dc311732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MaxPool2D:\n",
    "    def __init__(self, pool_size, stride=None):\n",
    "        # Handle pool_size as tuple or single value\n",
    "        if isinstance(pool_size, int):\n",
    "            self.pool_size = (pool_size, pool_size)\n",
    "        else:\n",
    "            self.pool_size = pool_size\n",
    "            \n",
    "        # If stride is not specified, set it equal to pool_size\n",
    "        if stride is None:\n",
    "            self.stride = self.pool_size\n",
    "        elif isinstance(stride, int):\n",
    "            self.stride = (stride, stride)\n",
    "        else:\n",
    "            self.stride = stride\n",
    "            \n",
    "        # For storing indices of max values for backpropagation\n",
    "        self.max_indices = None\n",
    "        self.x_shape = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of max pooling.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, height, width, channels) for NHWC format\n",
    "               \n",
    "        Returns:\n",
    "            Output tensor after max pooling\n",
    "        \"\"\"\n",
    "        self.x_shape = x.shape\n",
    "        batch_size, in_h, in_w, channels = x.shape\n",
    "        \n",
    "        # Calculate output dimensions\n",
    "        out_h = (in_h - self.pool_size[0]) // self.stride[0] + 1\n",
    "        out_w = (in_w - self.pool_size[1]) // self.stride[1] + 1\n",
    "        \n",
    "        # Initialize output tensor and indices of max values\n",
    "        output = np.zeros((batch_size, out_h, out_w, channels))\n",
    "        self.max_indices = np.zeros((batch_size, out_h, out_w, channels, 2), dtype=int)\n",
    "        \n",
    "        # Perform max pooling\n",
    "        for b in range(batch_size):\n",
    "            for i in range(out_h):\n",
    "                for j in range(out_w):\n",
    "                    # Define the current window position\n",
    "                    h_start = i * self.stride[0]\n",
    "                    h_end = h_start + self.pool_size[0]\n",
    "                    w_start = j * self.stride[1]\n",
    "                    w_end = w_start + self.pool_size[1]\n",
    "                    \n",
    "                    # Extract the current window\n",
    "                    window = x[b, h_start:h_end, w_start:w_end, :]\n",
    "                    \n",
    "                    # Find max values for each channel\n",
    "                    for c in range(channels):\n",
    "                        window_channel = window[:, :, c]\n",
    "                        max_idx = np.unravel_index(np.argmax(window_channel), window_channel.shape)\n",
    "                        output[b, i, j, c] = window_channel[max_idx]\n",
    "                        \n",
    "                        # Store indices for backpropagation\n",
    "                        self.max_indices[b, i, j, c] = [h_start + max_idx[0], w_start + max_idx[1]]\n",
    "                        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        \"\"\"\n",
    "        Backward pass of max pooling.\n",
    "        \n",
    "        Args:\n",
    "            delta: Gradient from the next layer, shape matching output of forward\n",
    "            \n",
    "        Returns:\n",
    "            Gradient with respect to input\n",
    "        \"\"\"\n",
    "        batch_size, out_h, out_w, channels = delta.shape\n",
    "        dx = np.zeros(self.x_shape)\n",
    "        \n",
    "        # Distribute gradients only to the positions that had max values during forward pass\n",
    "        for b in range(batch_size):\n",
    "            for i in range(out_h):\n",
    "                for j in range(out_w):\n",
    "                    for c in range(channels):\n",
    "                        h_idx, w_idx = self.max_indices[b, i, j, c]\n",
    "                        dx[b, h_idx, w_idx, c] += delta[b, i, j, c]\n",
    "                        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a6dad8-22c7-4e41-a142-f8ea08d99397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AveragePool2D:\n",
    "    def __init__(self, pool_size, stride=None):\n",
    "        # Handle pool_size as tuple or single value\n",
    "        if isinstance(pool_size, int):\n",
    "            self.pool_size = (pool_size, pool_size)\n",
    "        else:\n",
    "            self.pool_size = pool_size\n",
    "            \n",
    "        # If stride is not specified, set it equal to pool_size\n",
    "        if stride is None:\n",
    "            self.stride = self.pool_size\n",
    "        elif isinstance(stride, int):\n",
    "            self.stride = (stride, stride)\n",
    "        else:\n",
    "            self.stride = stride\n",
    "            \n",
    "        # For storing input shape for backpropagation\n",
    "        self.x_shape = None\n",
    "        self.pool_positions = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of average pooling.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, height, width, channels) for NHWC format\n",
    "               \n",
    "        Returns:\n",
    "            Output tensor after average pooling\n",
    "        \"\"\"\n",
    "        self.x_shape = x.shape\n",
    "        batch_size, in_h, in_w, channels = x.shape\n",
    "        \n",
    "        # Calculate output dimensions\n",
    "        out_h = (in_h - self.pool_size[0]) // self.stride[0] + 1\n",
    "        out_w = (in_w - self.pool_size[1]) // self.stride[1] + 1\n",
    "        \n",
    "        # Initialize output tensor\n",
    "        output = np.zeros((batch_size, out_h, out_w, channels))\n",
    "        \n",
    "        # Store pool positions for backprop\n",
    "        self.pool_positions = []\n",
    "        \n",
    "        # Perform average pooling\n",
    "        for b in range(batch_size):\n",
    "            for i in range(out_h):\n",
    "                for j in range(out_w):\n",
    "                    # Define the current window position\n",
    "                    h_start = i * self.stride[0]\n",
    "                    h_end = h_start + self.pool_size[0]\n",
    "                    w_start = j * self.stride[1]\n",
    "                    w_end = w_start + self.pool_size[1]\n",
    "                    \n",
    "                    # Store positions for backprop\n",
    "                    self.pool_positions.append((h_start, h_end, w_start, w_end))\n",
    "                    \n",
    "                    # Extract the current window and compute average\n",
    "                    window = x[b, h_start:h_end, w_start:w_end, :]\n",
    "                    output[b, i, j, :] = np.mean(window, axis=(0, 1))\n",
    "                        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        \"\"\"\n",
    "        Backward pass of average pooling.\n",
    "        \n",
    "        Args:\n",
    "            delta: Gradient from the next layer, shape matching output of forward\n",
    "            \n",
    "        Returns:\n",
    "            Gradient with respect to input\n",
    "        \"\"\"\n",
    "        batch_size, out_h, out_w, channels = delta.shape\n",
    "        dx = np.zeros(self.x_shape)\n",
    "        \n",
    "        # Distribute gradients evenly to all positions in each pooling window\n",
    "        for b in range(batch_size):\n",
    "            for i in range(out_h):\n",
    "                for j in range(out_w):\n",
    "                    # Get the positions of the current pooling window\n",
    "                    idx = i * out_w + j\n",
    "                    h_start, h_end, w_start, w_end = self.pool_positions[idx]\n",
    "                    \n",
    "                    # Number of elements in the pooling window\n",
    "                    pool_size = (h_end - h_start) * (w_end - w_start)\n",
    "                    \n",
    "                    # Distribute gradient evenly\n",
    "                    for c in range(channels):\n",
    "                        dx[b, h_start:h_end, w_start:w_end, c] += delta[b, i, j, c] / pool_size\n",
    "                        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "960c150c-716f-497f-8544-a917f2bb43ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self.x_shape = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of flatten operation.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, height, width, channels) for NHWC format\n",
    "            \n",
    "        Returns:\n",
    "            Flattened tensor of shape (batch_size, height*width*channels)\n",
    "        \"\"\"\n",
    "        self.x_shape = x.shape\n",
    "        batch_size = x.shape[0]\n",
    "        flattened_dim = np.prod(x.shape[1:])\n",
    "        \n",
    "        return x.reshape(batch_size, flattened_dim)\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        \"\"\"\n",
    "        Backward pass of flatten operation.\n",
    "        \n",
    "        Args:\n",
    "            delta: Gradient from the next layer, shape matching output of forward\n",
    "            \n",
    "        Returns:\n",
    "            Gradient with reshaped to match original input shape\n",
    "        \"\"\"\n",
    "        return delta.reshape(self.x_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "816aca2b-043d-4728-9762-440a6a2c6c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset...\n",
      "Creating CNN model...\n",
      "Starting training...\n",
      "Epoch 1/2, Batch 1/80, Loss: 2.3026\n",
      "Epoch 1/2, Batch 11/80, Loss: 2.3025\n",
      "Epoch 1/2, Batch 21/80, Loss: 2.3023\n",
      "Epoch 1/2, Batch 31/80, Loss: 2.3022\n",
      "Epoch 1/2, Batch 41/80, Loss: 2.3033\n",
      "Epoch 1/2, Batch 51/80, Loss: 2.3032\n",
      "Epoch 1/2, Batch 61/80, Loss: 2.3030\n",
      "Epoch 1/2, Batch 71/80, Loss: 2.3031\n",
      "Epoch 1/2, Train Loss: 2.3025, Val Loss: 2.3025, Train Acc: 0.1125, Val Acc: 0.1250\n",
      "Epoch 2/2, Batch 1/80, Loss: 2.3001\n",
      "Epoch 2/2, Batch 11/80, Loss: 2.3003\n",
      "Epoch 2/2, Batch 21/80, Loss: 2.3037\n",
      "Epoch 2/2, Batch 31/80, Loss: 2.2987\n",
      "Epoch 2/2, Batch 41/80, Loss: 2.2997\n",
      "Epoch 2/2, Batch 51/80, Loss: 2.2984\n",
      "Epoch 2/2, Batch 61/80, Loss: 2.3070\n",
      "Epoch 2/2, Batch 71/80, Loss: 2.3039\n",
      "Epoch 2/2, Train Loss: 2.3020, Val Loss: 2.3024, Train Acc: 0.1125, Val Acc: 0.1250\n",
      "Test accuracy: 0.1000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of ReLU activation.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            Output after ReLU activation\n",
    "        \"\"\"\n",
    "        self.mask = (x > 0)\n",
    "        return x * self.mask\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        \"\"\"\n",
    "        Backward pass of ReLU activation.\n",
    "        \n",
    "        Args:\n",
    "            delta: Gradient from the next layer\n",
    "            \n",
    "        Returns:\n",
    "            Gradient with respect to input\n",
    "        \"\"\"\n",
    "        return delta * self.mask\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of softmax activation.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            Output after softmax activation\n",
    "        \"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        self.output = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        \"\"\"\n",
    "        Backward pass of softmax activation.\n",
    "        \n",
    "        Args:\n",
    "            delta: Gradient from the next layer\n",
    "            \n",
    "        Returns:\n",
    "            Gradient with respect to input\n",
    "        \"\"\"\n",
    "        # For cross-entropy loss, this is typically simplified\n",
    "        return delta\n",
    "\n",
    "class Dense:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.bias = np.zeros((1, output_dim))\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of fully connected layer.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        return np.dot(x, self.weights) + self.bias\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        \"\"\"\n",
    "        Backward pass of fully connected layer.\n",
    "        \n",
    "        Args:\n",
    "            delta: Gradient from the next layer\n",
    "            \n",
    "        Returns:\n",
    "            Gradient with respect to input, weights, and bias\n",
    "        \"\"\"\n",
    "        dx = np.dot(delta, self.weights.T)\n",
    "        dw = np.dot(self.x.T, delta)\n",
    "        db = np.sum(delta, axis=0, keepdims=True)\n",
    "        return dx, dw, db\n",
    "    \n",
    "    def update(self, dw, db, learning_rate):\n",
    "        \"\"\"\n",
    "        Update weights and biases.\n",
    "        \n",
    "        Args:\n",
    "            dw: Weight gradients\n",
    "            db: Bias gradients\n",
    "            learning_rate: Learning rate for parameter update\n",
    "        \"\"\"\n",
    "        self.weights -= learning_rate * dw\n",
    "        self.bias -= learning_rate * db\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "        self.y_true = None\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Forward pass of cross-entropy loss.\n",
    "        \n",
    "        Args:\n",
    "            y_pred: Predicted probabilities\n",
    "            y_true: True labels (one-hot encoded)\n",
    "            \n",
    "        Returns:\n",
    "            Cross-entropy loss\n",
    "        \"\"\"\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        \n",
    "        # Add small epsilon to avoid log(0)\n",
    "        eps = 1e-10\n",
    "        y_pred_safe = np.clip(y_pred, eps, 1 - eps)\n",
    "        \n",
    "        loss = -np.sum(y_true * np.log(y_pred_safe)) / y_true.shape[0]\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Backward pass of cross-entropy loss.\n",
    "        \n",
    "        Returns:\n",
    "            Gradient with respect to predicted probabilities\n",
    "        \"\"\"\n",
    "        # With softmax, this simplifies to (y_pred - y_true)\n",
    "        return (self.y_pred - self.y_true) / self.y_true.shape[0]\n",
    "\n",
    "class Scratch2dCNNClassifier:\n",
    "    def __init__(self):\n",
    "        # Define the model architecture\n",
    "        self.conv1 = Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = ReLU()\n",
    "        self.pool1 = MaxPool2D(pool_size=2)\n",
    "        \n",
    "        self.conv2 = Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = ReLU()\n",
    "        self.pool2 = MaxPool2D(pool_size=2)\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        # 7x7 is the size after two 2x2 max pooling operations on 28x28 input\n",
    "        self.fc1 = Dense(7*7*64, 128)\n",
    "        self.relu3 = ReLU()\n",
    "        \n",
    "        self.fc2 = Dense(128, 10)\n",
    "        self.softmax = Softmax()\n",
    "        \n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the entire network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input images\n",
    "            \n",
    "        Returns:\n",
    "            Predicted probabilities\n",
    "        \"\"\"\n",
    "        x = self.conv1.forward(x)\n",
    "        x = self.relu1.forward(x)\n",
    "        x = self.pool1.forward(x)\n",
    "        \n",
    "        x = self.conv2.forward(x)\n",
    "        x = self.relu2.forward(x)\n",
    "        x = self.pool2.forward(x)\n",
    "        \n",
    "        x = self.flatten.forward(x)\n",
    "        \n",
    "        x = self.fc1.forward(x)\n",
    "        x = self.relu3.forward(x)\n",
    "        \n",
    "        x = self.fc2.forward(x)\n",
    "        x = self.softmax.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        \"\"\"\n",
    "        Backward pass through the entire network.\n",
    "        \n",
    "        Args:\n",
    "            y_true: True labels (one-hot encoded)\n",
    "            \n",
    "        Returns:\n",
    "            Loss value\n",
    "        \"\"\"\n",
    "        # Calculate loss\n",
    "        loss = self.loss_fn.forward(self.softmax.output, y_true)\n",
    "        \n",
    "        # Backward pass through the network\n",
    "        delta = self.loss_fn.backward()\n",
    "        \n",
    "        delta = self.softmax.backward(delta)\n",
    "        delta, dw_fc2, db_fc2 = self.fc2.backward(delta)\n",
    "        \n",
    "        delta = self.relu3.backward(delta)\n",
    "        delta, dw_fc1, db_fc1 = self.fc1.backward(delta)\n",
    "        \n",
    "        delta = self.flatten.backward(delta)\n",
    "        \n",
    "        delta = self.pool2.backward(delta)\n",
    "        delta = self.relu2.backward(delta)\n",
    "        delta, dw_conv2, db_conv2 = self.conv2.backward(delta)\n",
    "        \n",
    "        delta = self.pool1.backward(delta)\n",
    "        delta = self.relu1.backward(delta)\n",
    "        delta, dw_conv1, db_conv1 = self.conv1.backward(delta)\n",
    "        \n",
    "        # Update parameters\n",
    "        learning_rate = 0.01\n",
    "        self.fc2.update(dw_fc2, db_fc2, learning_rate)\n",
    "        self.fc1.update(dw_fc1, db_fc1, learning_rate)\n",
    "        self.conv2.update(dw_conv2, db_conv2, learning_rate)\n",
    "        self.conv1.update(dw_conv1, db_conv1, learning_rate)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def calculate_accuracy(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Calculate accuracy.\n",
    "        \n",
    "        Args:\n",
    "            y_pred: Predicted probabilities\n",
    "            y_true: True labels (one-hot encoded)\n",
    "            \n",
    "        Returns:\n",
    "            Accuracy\n",
    "        \"\"\"\n",
    "        pred_classes = np.argmax(y_pred, axis=1)\n",
    "        true_classes = np.argmax(y_true, axis=1)\n",
    "        \n",
    "        return np.mean(pred_classes == true_classes)\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, batch_size=32, epochs=5):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training images\n",
    "            y_train: Training labels (one-hot encoded)\n",
    "            X_val: Validation images\n",
    "            y_val: Validation labels (one-hot encoded)\n",
    "            batch_size: Batch size for training\n",
    "            epochs: Number of training epochs\n",
    "            \n",
    "        Returns:\n",
    "            Training history\n",
    "        \"\"\"\n",
    "        n_samples = X_train.shape[0]\n",
    "        n_batches = n_samples // batch_size\n",
    "        \n",
    "        history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_acc': []\n",
    "        }\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_train_shuffled = X_train[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for batch in range(n_batches):\n",
    "                start_idx = batch * batch_size\n",
    "                end_idx = (batch + 1) * batch_size\n",
    "                \n",
    "                X_batch = X_train_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_train_shuffled[start_idx:end_idx]\n",
    "                \n",
    "                # Forward pass\n",
    "                y_pred = self.forward(X_batch)\n",
    "                \n",
    "                # Backward pass\n",
    "                batch_loss = self.backward(y_batch)\n",
    "                epoch_loss += batch_loss\n",
    "                \n",
    "                # Print progress\n",
    "                if batch % 10 == 0:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs}, Batch {batch+1}/{n_batches}, Loss: {batch_loss:.4f}\")\n",
    "                \n",
    "            # Calculate training accuracy\n",
    "            y_pred_train = self.forward(X_train)\n",
    "            train_acc = self.calculate_accuracy(y_pred_train, y_train)\n",
    "            \n",
    "            # Calculate validation loss and accuracy\n",
    "            y_pred_val = self.forward(X_val)\n",
    "            val_loss = self.loss_fn.forward(y_pred_val, y_val)\n",
    "            val_acc = self.calculate_accuracy(y_pred_val, y_val)\n",
    "            \n",
    "            # Update history\n",
    "            history['train_loss'].append(epoch_loss / n_batches)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss/n_batches:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on new data.\n",
    "        \n",
    "        Args:\n",
    "            X: Input images\n",
    "            \n",
    "        Returns:\n",
    "            Predicted class indices\n",
    "        \"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Run training on MNIST dataset\n",
    "def train_mnist_cnn():\n",
    "    # Load MNIST dataset\n",
    "    print(\"Loading MNIST dataset...\")\n",
    "    mnist = fetch_openml('mnist_784', version=1, cache=True,parser='auto' )\n",
    "    \n",
    "    # Prepare data\n",
    "    num_samples = 2000\n",
    "    X = mnist.data.astype('float32').values[:num_samples].reshape(-1, 28, 28, 1) / 255.0\n",
    "    y = mnist.target.astype('int64').values[:num_samples]\n",
    "    # X = mnist.data.astype('float32').values.reshape(-1, 28, 28, 1) / 255.0\n",
    "    # y = mnist.target.astype('int64').values\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y_one_hot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create and train model\n",
    "    print(\"Creating CNN model...\")\n",
    "    model = Scratch2dCNNClassifier()\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    history = model.train(X_train, y_train, X_val, y_val, batch_size=16, epochs=2)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = model.forward(X_test)\n",
    "    test_acc = model.calculate_accuracy(y_pred, y_test)\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    return model, history, test_acc\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, history, test_acc = train_mnist_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6748c2f2-e92f-40b6-a7e9-97feb42539e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset...\n",
      "Creating LeNet model...\n",
      "Starting training...\n",
      "Epoch 1/2, Batch 1/80, Loss: 2.3026\n",
      "Epoch 1/2, Batch 11/80, Loss: 2.3024\n",
      "Epoch 1/2, Batch 21/80, Loss: 2.3036\n",
      "Epoch 1/2, Batch 31/80, Loss: 2.3029\n",
      "Epoch 1/2, Batch 41/80, Loss: 2.3006\n",
      "Epoch 1/2, Batch 51/80, Loss: 2.3022\n",
      "Epoch 1/2, Batch 61/80, Loss: 2.3050\n",
      "Epoch 1/2, Batch 71/80, Loss: 2.3024\n",
      "Epoch 1/2, Train Loss: 2.3026, Val Loss: 2.3025, Train Acc: 0.1125, Val Acc: 0.1250\n",
      "Epoch 2/2, Batch 1/80, Loss: 2.3048\n",
      "Epoch 2/2, Batch 11/80, Loss: 2.3015\n",
      "Epoch 2/2, Batch 21/80, Loss: 2.2963\n",
      "Epoch 2/2, Batch 31/80, Loss: 2.3052\n",
      "Epoch 2/2, Batch 41/80, Loss: 2.3044\n",
      "Epoch 2/2, Batch 51/80, Loss: 2.3026\n",
      "Epoch 2/2, Batch 61/80, Loss: 2.3004\n",
      "Epoch 2/2, Batch 71/80, Loss: 2.2964\n",
      "Epoch 2/2, Train Loss: 2.3020, Val Loss: 2.3025, Train Acc: 0.1125, Val Acc: 0.1250\n",
      "Test accuracy: 0.1000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class LeNet:\n",
    "    def __init__(self):\n",
    "        # Define the model architecture based on LeNet but adapted for MNIST (28x28)\n",
    "        \n",
    "        # Layer 1: Convolution layer (C1)\n",
    "        # 6 output channels, 5x5 filter size, stride 1\n",
    "        self.conv1 = Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu1 = ReLU()\n",
    "        \n",
    "        # Layer 2: Pooling (S2)\n",
    "        self.pool1 = MaxPool2D(pool_size=2, stride=2)\n",
    "        \n",
    "        # Layer 3: Convolution layer (C3)\n",
    "        # 16 output channels, 5x5 filter size, stride 1\n",
    "        self.conv2 = Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu2 = ReLU()\n",
    "        \n",
    "        # Layer 4: Pooling (S4)\n",
    "        self.pool2 = MaxPool2D(pool_size=2, stride=2)\n",
    "        \n",
    "        # Layer 5: Flatten\n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        # Calculate the size after convolutions and pooling\n",
    "        # 28x28 -> Conv1(5x5) -> 24x24 -> Pool1(2x2) -> 12x12\n",
    "        # 12x12 -> Conv2(5x5) -> 8x8 -> Pool2(2x2) -> 4x4\n",
    "        # So, 4x4x16 = 256 features after flatten\n",
    "        \n",
    "        # Layer 6: Fully connected layer (F5)\n",
    "        self.fc1 = Dense(input_dim=4*4*16, output_dim=120)\n",
    "        self.relu3 = ReLU()\n",
    "        \n",
    "        # Layer 7: Fully connected layer (F6)\n",
    "        self.fc2 = Dense(input_dim=120, output_dim=84)\n",
    "        self.relu4 = ReLU()\n",
    "        \n",
    "        # Layer 8: Output layer (F7)\n",
    "        self.fc3 = Dense(input_dim=84, output_dim=10)\n",
    "        self.softmax = Softmax()\n",
    "        \n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the LeNet architecture.\n",
    "        \n",
    "        Args:\n",
    "            x: Input images (batch_size, height, width, channels)\n",
    "            \n",
    "        Returns:\n",
    "            Predicted probabilities for each class\n",
    "        \"\"\"\n",
    "        # Layer 1: Convolution\n",
    "        x = self.conv1.forward(x)\n",
    "        x = self.relu1.forward(x)\n",
    "        \n",
    "        # Layer 2: Pooling\n",
    "        x = self.pool1.forward(x)\n",
    "        \n",
    "        # Layer 3: Convolution\n",
    "        x = self.conv2.forward(x)\n",
    "        x = self.relu2.forward(x)\n",
    "        \n",
    "        # Layer 4: Pooling\n",
    "        x = self.pool2.forward(x)\n",
    "        \n",
    "        # Layer 5: Flatten\n",
    "        x = self.flatten.forward(x)\n",
    "        \n",
    "        # Layer 6: Fully connected\n",
    "        x = self.fc1.forward(x)\n",
    "        x = self.relu3.forward(x)\n",
    "        \n",
    "        # Layer 7: Fully connected\n",
    "        x = self.fc2.forward(x)\n",
    "        x = self.relu4.forward(x)\n",
    "        \n",
    "        # Layer 8: Output layer\n",
    "        x = self.fc3.forward(x)\n",
    "        x = self.softmax.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        \"\"\"\n",
    "        Backward pass through the LeNet architecture.\n",
    "        \n",
    "        Args:\n",
    "            y_true: True labels (one-hot encoded)\n",
    "            \n",
    "        Returns:\n",
    "            Loss value\n",
    "        \"\"\"\n",
    "        # Calculate loss\n",
    "        loss = self.loss_fn.forward(self.softmax.output, y_true)\n",
    "        \n",
    "        # Backward pass through the network\n",
    "        delta = self.loss_fn.backward()\n",
    "        \n",
    "        # Layer 8: Output layer\n",
    "        delta = self.softmax.backward(delta)\n",
    "        delta, dw_fc3, db_fc3 = self.fc3.backward(delta)\n",
    "        \n",
    "        # Layer 7: Fully connected\n",
    "        delta = self.relu4.backward(delta)\n",
    "        delta, dw_fc2, db_fc2 = self.fc2.backward(delta)\n",
    "        \n",
    "        # Layer 6: Fully connected\n",
    "        delta = self.relu3.backward(delta)\n",
    "        delta, dw_fc1, db_fc1 = self.fc1.backward(delta)\n",
    "        \n",
    "        # Layer 5: Flatten\n",
    "        delta = self.flatten.backward(delta)\n",
    "        \n",
    "        # Layer 4: Pooling\n",
    "        delta = self.pool2.backward(delta)\n",
    "        \n",
    "        # Layer 3: Convolution\n",
    "        delta = self.relu2.backward(delta)\n",
    "        delta, dw_conv2, db_conv2 = self.conv2.backward(delta)\n",
    "        \n",
    "        # Layer 2: Pooling\n",
    "        delta = self.pool1.backward(delta)\n",
    "        \n",
    "        # Layer 1: Convolution\n",
    "        delta = self.relu1.backward(delta)\n",
    "        delta, dw_conv1, db_conv1 = self.conv1.backward(delta)\n",
    "        \n",
    "        # Update parameters\n",
    "        learning_rate = 0.01\n",
    "        \n",
    "        self.fc3.update(dw_fc3, db_fc3, learning_rate)\n",
    "        self.fc2.update(dw_fc2, db_fc2, learning_rate)\n",
    "        self.fc1.update(dw_fc1, db_fc1, learning_rate)\n",
    "        self.conv2.update(dw_conv2, db_conv2, learning_rate)\n",
    "        self.conv1.update(dw_conv1, db_conv1, learning_rate)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def calculate_accuracy(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Calculate accuracy.\n",
    "        \n",
    "        Args:\n",
    "            y_pred: Predicted probabilities\n",
    "            y_true: True labels (one-hot encoded)\n",
    "            \n",
    "        Returns:\n",
    "            Accuracy\n",
    "        \"\"\"\n",
    "        pred_classes = np.argmax(y_pred, axis=1)\n",
    "        true_classes = np.argmax(y_true, axis=1)\n",
    "        \n",
    "        return np.mean(pred_classes == true_classes)\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, batch_size=32, epochs=5):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training images\n",
    "            y_train: Training labels (one-hot encoded)\n",
    "            X_val: Validation images\n",
    "            y_val: Validation labels (one-hot encoded)\n",
    "            batch_size: Batch size for training\n",
    "            epochs: Number of training epochs\n",
    "            \n",
    "        Returns:\n",
    "            Training history\n",
    "        \"\"\"\n",
    "        n_samples = X_train.shape[0]\n",
    "        n_batches = n_samples // batch_size\n",
    "        \n",
    "        history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_acc': []\n",
    "        }\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_train_shuffled = X_train[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for batch in range(n_batches):\n",
    "                start_idx = batch * batch_size\n",
    "                end_idx = (batch + 1) * batch_size\n",
    "                \n",
    "                X_batch = X_train_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_train_shuffled[start_idx:end_idx]\n",
    "                \n",
    "                # Forward pass\n",
    "                y_pred = self.forward(X_batch)\n",
    "                \n",
    "                # Backward pass\n",
    "                batch_loss = self.backward(y_batch)\n",
    "                epoch_loss += batch_loss\n",
    "                \n",
    "                # Print progress\n",
    "                if batch % 10 == 0:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs}, Batch {batch+1}/{n_batches}, Loss: {batch_loss:.4f}\")\n",
    "                \n",
    "            # Calculate training accuracy\n",
    "            y_pred_train = self.forward(X_train)\n",
    "            train_acc = self.calculate_accuracy(y_pred_train, y_train)\n",
    "            \n",
    "            # Calculate validation loss and accuracy\n",
    "            y_pred_val = self.forward(X_val)\n",
    "            val_loss = self.loss_fn.forward(y_pred_val, y_val)\n",
    "            val_acc = self.calculate_accuracy(y_pred_val, y_val)\n",
    "            \n",
    "            # Update history\n",
    "            history['train_loss'].append(epoch_loss / n_batches)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss/n_batches:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on new data.\n",
    "        \n",
    "        Args:\n",
    "            X: Input images\n",
    "            \n",
    "        Returns:\n",
    "            Predicted class indices\n",
    "        \"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Run training on MNIST dataset with LeNet\n",
    "def train_mnist_lenet():\n",
    "    # Load MNIST dataset\n",
    "    print(\"Loading MNIST dataset...\")\n",
    "    mnist = fetch_openml('mnist_784', version=1, cache=True, parser='auto')\n",
    "    \n",
    "    # Prepare data\n",
    "    num_samples = 2000\n",
    "    X = mnist.data.astype('float32').values[:num_samples].reshape(-1, 28, 28, 1) / 255.0\n",
    "    y = mnist.target.astype('int64').values[:num_samples]\n",
    "    # X = mnist.data.astype('float32').values.reshape(-1, 28, 28, 1) / 255.0\n",
    "    # y = mnist.target.astype('int64').values\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y_one_hot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create and train model\n",
    "    print(\"Creating LeNet model...\")\n",
    "    model = LeNet()\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    history = model.train(X_train, y_train, X_val, y_val, batch_size=16, epochs=2)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = model.forward(X_test)\n",
    "    test_acc = model.calculate_accuracy(y_pred, y_test)\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    return model, history, test_acc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, history, test_acc = train_mnist_lenet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca5bcc2-6879-4fb1-8cfc-ddf51df0b47d",
   "metadata": {},
   "source": [
    "### Problem 9: Research into Famous Image Recognition Models\n",
    "\n",
    "Here's a brief overview of famous CNN architectures for image recognition:\n",
    "\n",
    "Popular CNN architectures that are commonly available in deep learning frameworks include:\n",
    "\n",
    "- **AlexNet (2012)**  \n",
    "  The breakthrough model that won the ImageNet competition in 2012, significantly outperforming other approaches. It used ReLU activation, dropout for regularization, and data augmentation.\n",
    "\n",
    "- **VGG16/VGG19 (2014)**  \n",
    "  Known for its simplicity and depth, using small 33 convolution filters throughout the network with increasing depth.\n",
    "\n",
    "- **GoogLeNet/Inception (2014)**  \n",
    "  Introduced the inception module that uses parallel convolutions of different sizes to capture features at different scales.\n",
    "\n",
    "- **ResNet (2015)**  \n",
    "  Introduced residual connections (skip connections) to enable training of very deep networks (up to 152 layers), addressing the vanishing gradient problem.\n",
    "\n",
    "- **DenseNet (2017)**  \n",
    "  Each layer connects to every other layer in a feed-forward fashion, improving feature reuse and parameter efficiency.\n",
    "\n",
    "- **MobileNet (2017)**  \n",
    "  Designed for mobile/embedded applications with depthwise separable convolutions to reduce parameters and computation.\n",
    "\n",
    "- **EfficientNet (2019)**  \n",
    "  Uses compound scaling to uniformly scale network width, depth, and resolution for better efficiency.\n",
    "\n",
    "- **Vision Transformer (ViT) (2020)**  \n",
    "  Adapts transformer architecture from NLP to image classification by splitting images into patches.\n",
    "\n",
    "---\n",
    "\n",
    "Most deep learning frameworks provide pre-trained versions of these models:\n",
    "\n",
    "- **PyTorch**: `torchvision.models`  \n",
    "- **Keras/TensorFlow**: `keras.applications`  \n",
    "- **TensorFlow**: `tensorflow.keras.applications`\n",
    "\n",
    "These pre-trained models can be used for **transfer learning** on custom datasets, where the convolutional base is frozen and a new classifier head is trained on the target task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a348a1-f24f-4b8b-90a2-6e79c3c28a67",
   "metadata": {},
   "source": [
    "### Problem 10: Calculating the Output Size and Number of Parameters\n",
    "\n",
    "Let's calculate the output size and parameters for the given convolutional layers:\n",
    "\n",
    "#### Case 1:\n",
    "- **Input size**: 144144, 3 channels  \n",
    "- **Filter size**: 33, 6 channels  \n",
    "- **Stride**: 1  \n",
    "- **Padding**: None  \n",
    "\n",
    "**Output size calculation**:\n",
    "- Output height = (144 + 20 - 3) / 1 + 1 = 142  \n",
    "- Output width = (144 + 20 - 3) / 1 + 1 = 142  \n",
    "- **Output shape**: (142, 142, 6)\n",
    "\n",
    "**Number of parameters**:\n",
    "- Each filter: 3  3  3 = 27 weights (for each of the 6 output channels)  \n",
    "- Total weights: 27  6 = 162  \n",
    "- Bias terms: 1  6 = 6  \n",
    "- **Total parameters**: 162 + 6 = 168  \n",
    "\n",
    "---\n",
    "\n",
    "#### Case 2:\n",
    "- **Input size**: 6060, 24 channels  \n",
    "- **Filter size**: 33, 48 channels  \n",
    "- **Stride**: 1  \n",
    "- **Padding**: None  \n",
    "\n",
    "**Output size calculation**:\n",
    "- Output height = (60 + 20 - 3) / 1 + 1 = 58  \n",
    "- Output width = (60 + 20 - 3) / 1 + 1 = 58  \n",
    "- **Output shape**: (58, 58, 48)\n",
    "\n",
    "**Number of parameters**:\n",
    "- Each filter: 3  3  24 = 216 weights (for each of the 48 output channels)  \n",
    "- Total weights: 216  48 = 10,368  \n",
    "- Bias terms: 1  48 = 48  \n",
    "- **Total parameters**: 10,368 + 48 = 10,416  \n",
    "\n",
    "---\n",
    "\n",
    "#### Case 3:\n",
    "- **Input size**: 2020, 10 channels  \n",
    "- **Filter size**: 33, 20 channels  \n",
    "- **Stride**: 2  \n",
    "- **Padding**: None  \n",
    "\n",
    "**Output size calculation**:\n",
    "- Output height = (20 + 20 - 3) / 2 + 1 = 9.5  9 (truncated)  \n",
    "- Output width = (20 + 20 - 3) / 2 + 1 = 9.5  9 (truncated)  \n",
    "- **Output shape**: (9, 9, 20)\n",
    "\n",
    "**Number of parameters**:\n",
    "- Each filter: 3  3  10 = 90 weights (for each of the 20 output channels)  \n",
    "- Total weights: 90  20 = 1,800  \n",
    "- Bias terms: 1  20 = 20  \n",
    "- **Total parameters**: 1,800 + 20 = 1,820  \n",
    "\n",
    "In the last case, with a stride of 2, we can't perfectly cover the input. The framework would typically ignore the extra pixels at the edges, resulting in a 99 output feature map.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81254019-dd94-4d5b-8ab7-102b212d1367",
   "metadata": {},
   "source": [
    "### Problem 11: Investigation into Filter Size\n",
    "\n",
    "#### Why are 33 filters more commonly used than larger ones like 77?\n",
    "\n",
    "There are several reasons why 33 filters are preferred over larger filters like 77:\n",
    "\n",
    "1. **Parameter Efficiency**:  \n",
    "   Stacking multiple 33 convolutions can achieve the same receptive field as a single large filter while using fewer parameters. For example, three stacked 33 filters have a 77 effective receptive field but only 27 parameters per channel (333), compared to 49 parameters (77) for a single 77 filter.\n",
    "\n",
    "2. **More Non-linearity**:  \n",
    "   With stacked smaller filters, we can include more activation functions (ReLU) between layers, introducing more non-linearity to the network. This helps the network learn more complex features.\n",
    "\n",
    "3. **Computational Efficiency**:  \n",
    "   Multiple 33 convolutions are often more computationally efficient than a single large convolution due to optimization in modern deep learning frameworks.\n",
    "\n",
    "4. **Regularization Effect**:  \n",
    "   The additional non-linearities added by stacking smaller filters act as implicit regularization, improving the network's generalization.\n",
    "\n",
    "5. **Flexibility**:  \n",
    "   Using stacked small filters allows the network to learn hierarchical representations, where early layers capture simple features and later layers combine them into complex ones.\n",
    "\n",
    "6. **Historical Success**:  \n",
    "   The VGG networks demonstrated the effectiveness of using exclusively 33 filters, influencing subsequent architectures.\n",
    "\n",
    "---\n",
    "\n",
    "#### The Effect of a 11 Filter with No Height or Width\n",
    "\n",
    "Despite their small size, 11 filters play important roles in CNN architectures:\n",
    "\n",
    "1. **Dimensionality Reduction**:  \n",
    "   The primary use is to reduce the number of channels/feature maps. This is often called a \"bottleneck layer\" and significantly reduces computational cost.\n",
    "\n",
    "2. **Cross-Channel Information Flow**:  \n",
    "   11 convolutions mix information across channels while maintaining spatial dimensions, essentially performing a pointwise transformation.\n",
    "\n",
    "3. **Adding Non-linearity**:  \n",
    "   When followed by an activation function, 11 convolutions add non-linearity to the network without changing spatial dimensions.\n",
    "\n",
    "4. **Network-in-Network Architecture**:  \n",
    "   They enable the creation of \"micro-networks\" within each convolution layer, increasing the representational power.\n",
    "\n",
    "5. **Parameter Reduction**:  \n",
    "   In architectures like Inception modules, they're used before larger filters to reduce input channels, decreasing the total parameter count.\n",
    "\n",
    "6. **Fixed Spatial Resolution**:  \n",
    "   When you want to maintain spatial resolution but need to manipulate the feature space, 11 convolutions are ideal.\n",
    "\n",
    "7. **Computational Efficiency**:  \n",
    "   They require significantly fewer computations than larger filters, making networks more efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f52d5ff-e1ee-4558-8385-1dcebb31d20f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
